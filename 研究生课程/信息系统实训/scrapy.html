<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>scrapy - zronghui的博客</title><meta description="[TOC]"><meta property="og:type" content="blog"><meta property="og:title" content="zronghui"><meta property="og:url" content="https://zronghui.github.io/"><meta property="og:site_name" content="zronghui"><meta property="og:description" content="[TOC]"><meta property="og:image" content="https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png# https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg"><meta property="article:published_time" content="2020-03-10T03:26:04.000Z"><meta property="article:modified_time" content="2020-05-12T14:34:39.000Z"><meta property="article:author" content="removeif"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png# https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zronghui.github.io/"},"headline":"zronghui","image":["https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png#https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg# https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@latest/img/avatar.jpg"],"datePublished":"2020-03-10T03:26:04.000Z","dateModified":"2020-05-12T14:34:39.000Z","author":{"@type":"Person","name":"removeif"},"description":"[TOC]"}</script><link rel="alternative" href="/atom.xml" title="zronghui的博客" type="application/atom+xml"><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@latest/img/wico.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://www.wntool.com/logo/image.php?output=png?fsize=35&amp;font=Snow.ttf&amp;text=zronghui&amp;mirror=No&amp;color=9933FF&amp;vcolor=3333FF&amp;bgcolor=FFFFFF&amp;alpha=yes&amp;output=png&amp;spacing=5&amp;shadow=no&amp;transparent=yes&amp;icon=no&amp;iconic=&amp;top_spacing=5&amp;left_spacing=6&amp;icon_size=48" alt="zronghui的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/media">影音</a><a class="navbar-item" href="/album">相册</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zronghui"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-03-10T03:26:04.000Z">2020-03-10</time><a class="commentCountImg" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html#comment-container"><span class="display-none-class">865cead3837eb7e055c28ed7c9679381</span><img class="not-gallery-item" src="/img/chat.svg"> <span class="commentCount" id="865cead3837eb7e055c28ed7c9679381"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a><span> / </span><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/">信息系统实训</a></span><span class="level-item">an hour 读完 (大约 6830 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">scrapy</h1><div class="content"><p>[TOC]</p>
<a id="more"></a>

<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html">Scrapy入门教程 — Scrapy 0.24.6 文档</a></p>
<h2 id="helloScrapy"><a href="#helloScrapy" class="headerlink" title="helloScrapy"></a>helloScrapy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line">scrapy startproject helloScrapy</span><br><span class="line">scrapy genspider volmoe volmoe.com</span><br><span class="line">scrapy genspider douban douban.com</span><br></pre></td></tr></table></figure>



<h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><p>300 是权重，决定多个 pipeline 执行的顺序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ROBOTSTXT_OBEY = True</span></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'helloScrapy.pipelines.HelloscrapyPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理爬虫所爬取到的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file = open(<span class="string">'books.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 现将item数据转为字典类型，再将其保存为json文件</span></span><br><span class="line">        text = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">'\n'</span></span><br><span class="line">        <span class="comment"># 写入本地</span></span><br><span class="line">        self.file.write(text)</span><br><span class="line">        <span class="comment"># 会将item打印到屏幕上，方便观察</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫关闭时所执行的操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>



<h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    book_name = scrapy.Field()</span><br><span class="line">    book_url = scrapy.Field()</span><br><span class="line">    book_desc = scrapy.Field()</span><br></pre></td></tr></table></figure>



<h3 id="Spider-volmoe-py"><a href="#Spider-volmoe-py" class="headerlink" title="Spider/volmoe.py"></a>Spider/volmoe.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> zhconv <span class="keyword">import</span> convert</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helloScrapy.items <span class="keyword">import</span> BookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolmoeSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'volmoe'</span></span><br><span class="line">    allowed_domains = [<span class="string">'volmoe.com'</span>]</span><br><span class="line">    <span class="comment"># start_urls = [f'https://volmoe.com/l/all,all,all,sortpoint,all,all/&#123;i&#125;.htm' for i in range(1, 2)]</span></span><br><span class="line">    start_urls = [<span class="string">f'https://volmoe.com/l/all,all,all,sortpoint,all,all/<span class="subst">&#123;i&#125;</span>.htm'</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">473</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自动调用 parse ，解析 item URL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        itemUrlList = list(set(response.xpath(<span class="string">'/html/body/div[7]/table'</span>) \</span><br><span class="line">                               .xpath(<span class="string">'//a/@href'</span>) \</span><br><span class="line">                               .re(<span class="string">r'https://volmoe.com/c/\d+.htm'</span>)))</span><br><span class="line">        <span class="keyword">for</span> itemUrl <span class="keyword">in</span> itemUrlList:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=itemUrl, callback=self.parse_item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 手动调用 parse_item ，解析 item</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            item = BookItem()</span><br><span class="line">            item[<span class="string">'book_url'</span>] = response.url</span><br><span class="line">            item[<span class="string">'book_name'</span>] = convert(response.xpath(<span class="string">'//div/b/text()'</span>).extract()[<span class="number">0</span>], <span class="string">'zh-cn'</span>)</span><br><span class="line">            item[<span class="string">'book_desc'</span>] = convert(response.xpath(<span class="string">'//*[@id="desc_text"]/text()'</span>).extract()[<span class="number">0</span>].strip(), <span class="string">'zh-cn'</span>)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure>

<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol>
<li><p>爬取 列表页+详情页 用 rules 更方便，但是我用了，没有生效，所以手动指定函数处理</p>
</li>
<li><p>直接在网页上的 xpath 可能会与爬虫请求到的结构不同，所以可以用 scrapy shell “url”， view(response), 在网页上使用 xpath finder , xpath helper 查看指定元素的 xpath</p>
</li>
<li><p>scrapy genspider mydomain mydomain.com，最后的 mydomain.com 不用加 http 和 com 后面的/, 如 <del><a href="http://mydomain.com/">http://mydomain.com/</a></del></p>
</li>
<li><p>借助 <a href="https://github.com/further-reading/scrapy-gui">https://github.com/further-reading/scrapy-gui</a> 测试 CSS <del>xpath</del></p>
<img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" />

<ol start="5">
<li><p>获得的 URL 没有域名时：url = response.urljoin(next)  ?</p>
</li>
<li><p>可以在一个爬虫中用 custom_settings <img src="https://i.loli.net/2020/03/12/YLS5ftcphlDMTzx.png" alt="YLS5ftcphlDMTzx" style="zoom: 25%;" /></p>
</li>
<li><p>scrapy对request的URL去重 通过 yield scrapy.Request(url, self.parse, dont_filter=False) 里的 dont_filter（默认False） 实现</p>
</li>
<li><p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数运行爬虫，如：scrapy crawl spider1 -L WARNING</p>
</li>
<li><p>scrapy shell 设置 UA</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell 'http://www.gqzzw.com/type/bjsh' -s USER_AGENT='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
<pre><code>&lt;img src=&quot;https://i.loli.net/2020/03/12/H84sqghj3rNXpWv.jpg&quot; alt=&quot;H84sqghj3rNXpWv&quot; style=&quot;zoom:50%;&quot; /&gt;</code></pre><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h3 id="启动所有爬虫-代码启动"><a href="#启动所有爬虫-代码启动" class="headerlink" title="启动所有爬虫 代码启动"></a>启动所有爬虫 代码启动</h3><p><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html">Scrapy教程10- 动态配置爬虫 — scrapy-cookbook 0.2.2 文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pretty_errors</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helloScrapy.spiders.gqzzw <span class="keyword">import</span> GqzzwSpider</span><br><span class="line"></span><br><span class="line">pretty_errors.activate()</span><br><span class="line">lastPage = input(<span class="string">'lastPage: '</span>)</span><br><span class="line">zzname = input(<span class="string">'zzname: '</span>)</span><br><span class="line">process = CrawlerProcess(get_project_settings())</span><br><span class="line"><span class="comment"># 传入参数</span></span><br><span class="line">process.crawl(GqzzwSpider, lastPage=lastPage, zzname=zzname)</span><br><span class="line">process.start()  <span class="comment"># the script will block here until the crawling is finished</span></span><br></pre></td></tr></table></figure>





<h3 id="启动一个爬虫"><a href="#启动一个爬虫" class="headerlink" title="启动一个爬虫"></a>启动一个爬虫</h3><p>scrapy crawl volmoe</p>
<h3 id="事例-spider"><a href="#事例-spider" class="headerlink" title="事例 spider"></a>事例 spider</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</span><br><span class="line">            item = DmozItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'text()'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>



<h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>最简单存储爬取的数据的方式是使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/feed-exports.html#topics-feed-exports">Feed exports</a>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dmoz -o items.json</span><br></pre></td></tr></table></figure>

<p>该命令将采用 <a href="http://en.wikipedia.org/wiki/JSON">JSON</a> 格式对爬取的数据进行序列化，生成 <code>items.json</code> 文件。</p>
<p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的item做更多更为复杂的操作，您可以编写 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/item-pipeline.html#topics-item-pipeline">Item Pipeline</a> 。 </p>
<h3 id="在Shell中尝试Selector选择器"><a href="#在Shell中尝试Selector选择器" class="headerlink" title="在Shell中尝试Selector选择器"></a>在Shell中尝试Selector选择器</h3><p>为了介绍Selector的使用方法，接下来我们将要使用内置的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html#topics-shell">Scrapy shell</a> 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p>
<p>您需要进入项目的根目录，执行下列命令来启动shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&quot;</span><br></pre></td></tr></table></figure>

<p>注解</p>
<p>当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 <code>&amp;</code> 字符)会导致Scrapy运行失败。</p>
<p>shell的输出类似:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[ ... Scrapy log here ... ]</span><br><span class="line"></span><br><span class="line">2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;</span><br><span class="line">[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span><br><span class="line">[s]   view(response)    View response in a browser</span><br><span class="line"></span><br><span class="line">In [1]:</span><br></pre></td></tr></table></figure>

<p>当shell载入后，您将得到一个包含response数据的本地 <code>response</code> 变量。输入 <code>response.body</code> 将输出response的包体， 输出 <code>response.headers</code> 可以看到response的包头。</p>
<p>更为重要的是，当输入 <code>response.selector</code> 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 <code>response.selector.xpath()</code> 、 <code>response.selector.css()</code> 的 快捷方法(shortcut): <code>response.xpath()</code> 和 <code>response.css()</code> 。</p>
<p>同时，shell根据response提前初始化了变量 <code>sel</code> 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。</p>
<p>让我们来试试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: response.xpath(<span class="string">'//title'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [&lt;Selector xpath=<span class="string">'//title'</span> data=<span class="string">u'&lt;title&gt;Open Directory - Computers: Progr'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: response.xpath(<span class="string">'//title'</span>).extract()</span><br><span class="line">Out[<span class="number">2</span>]: [<span class="string">u'&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: response.xpath(<span class="string">'//title/text()'</span>)</span><br><span class="line">Out[<span class="number">3</span>]: [&lt;Selector xpath=<span class="string">'//title/text()'</span> data=<span class="string">u'Open Directory - Computers: Programming:'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: response.xpath(<span class="string">'//title/text()'</span>).extract()</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="string">u'Open Directory - Computers: Programming: Languages: Python: Books'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: response.xpath(<span class="string">'//title/text()'</span>).re(<span class="string">'(\w+):'</span>)</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="string">u'Computers'</span>, <span class="string">u'Programming'</span>, <span class="string">u'Languages'</span>, <span class="string">u'Python'</span>]</span><br></pre></td></tr></table></figure>

<h3 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h3><p>详见：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html">命令行工具(Command line tools) — Scrapy 0.24.6 文档</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以指定模板（怎么自定义模板？）</span></span><br><span class="line">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看 basic 模板</span></span><br><span class="line">scrapy genspider -d basic</span><br><span class="line"></span><br><span class="line">scrapy crawl myspider</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行contract检查, 运行没问题就好</span></span><br><span class="line">scrapy check [-l] &lt;spider&gt;</span><br><span class="line"></span><br><span class="line">scrapy list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 EDITOR 中设定的编辑器编辑给定的spider</span></span><br><span class="line">scrapy edit spider1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出（命令行？）</span></span><br><span class="line">scrapy fetch &lt;url&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ** 在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。</span></span><br><span class="line">scrapy view &lt;url&gt;</span><br><span class="line"></span><br><span class="line">scrapy shell [url]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。</span></span><br><span class="line">scrapy parse &lt;url&gt; [options]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取Scrapy的设定</span></span><br><span class="line">scrapy settings [options]</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get BOT_NAME</span></span><br><span class="line">scrapybot</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get DOWNLOAD_DELAY</span></span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在未创建项目的情况下，运行一个编写在Python文件中的spider。</span></span><br><span class="line">scrapy runspider &lt;spider_file.py&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy runspider myspider.py</span></span><br><span class="line">[ ... spider starts crawling ... ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。</span></span><br><span class="line">scrapy version [-v]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将项目部署到Scrapyd服务？</span></span><br><span class="line">scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行benchmark测试</span></span><br><span class="line">scrapy bench</span><br></pre></td></tr></table></figure>

<h3 id="items"><a href="#items" class="headerlink" title="items"></a>items</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个 item</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># item API和 dict API 非常相似</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一、与Item配合</span></span><br><span class="line"><span class="comment"># 创建item</span></span><br><span class="line">Product(name=<span class="string">'PC'</span>, price=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 获取字段的值</span></span><br><span class="line">p.get(<span class="string">'name'</span>)</span><br><span class="line">p.get(<span class="string">'name'</span>, <span class="string">'default'</span>)</span><br><span class="line">p[<span class="string">'name'</span>]</span><br><span class="line"><span class="string">'name'</span> <span class="keyword">in</span> product <span class="comment"># True  name 字段是否有填充</span></span><br><span class="line"><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields <span class="comment"># False  last_updated 是否是声明的字段</span></span><br><span class="line"><span class="comment"># 设置字段的值</span></span><br><span class="line">p[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></span><br><span class="line"><span class="comment"># 获取所有获取到的值</span></span><br><span class="line">p.keys() <span class="comment"># ['price', 'name']</span></span><br><span class="line">p.items() <span class="comment"># [('price', 1000), ('name', 'Desktop PC')]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他任务</span></span><br><span class="line"><span class="comment">#   复制item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product2 = Product(product)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product3 = product2.copy()</span><br><span class="line"><span class="comment">#   根据item创建字典(dict):</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dict(product) <span class="comment"># create a dict from all populated values</span></span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="number">1000</span>, <span class="string">'name'</span>: <span class="string">'Desktop PC'</span>&#125;</span><br><span class="line"><span class="comment">#   根据字典(dict)创建item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Product(&#123;<span class="string">'name'</span>: <span class="string">'Laptop PC'</span>, <span class="string">'price'</span>: <span class="number">1500</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩展Item</span></span><br><span class="line"><span class="comment">#   可以通过继承原始的Item来扩展item(添加更多的字段或者修改某些字段的元数据)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscountedProduct</span><span class="params">(Product)</span>:</span></span><br><span class="line">    discount_percent = scrapy.Field(serializer=str)</span><br><span class="line">    discount_expiration_date = scrapy.Field()</span><br><span class="line"><span class="comment"># Item对象</span></span><br><span class="line"><span class="comment"># 字段(Field)对象</span></span><br></pre></td></tr></table></figure>



<h3 id="选择器-css-xpath"><a href="#选择器-css-xpath" class="headerlink" title="选择器  css xpath"></a>选择器  css xpath</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//div/a/text()'</span>).extract()</span><br><span class="line">response.css(<span class="string">'div a::text'</span>).extract()</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">'//@class'</span>).extract()  <span class="comment"># 选取所有名为 class 的属性</span></span><br><span class="line">response.xpath(<span class="string">'//article/div[1]'</span>).extract()</span><br><span class="line">response.xpath(<span class="string">'//article/div[last()]'</span>).extract()</span><br><span class="line">response.xpath(<span class="string">'//article/div[last()-1]'</span>).extract()</span><br><span class="line">response.xpath(<span class="string">'//article/div[@lang]'</span>).extract() <span class="comment"># 包含 lang 属性</span></span><br><span class="line">response.xpath(<span class="string">"//article/div[@lang='eng']"</span>).extract() <span class="comment"># lang 属性为 eng</span></span><br><span class="line">response.xpath(<span class="string">'/div/*'</span>).extract() <span class="comment"># 所有子节点</span></span><br><span class="line">response.xpath(<span class="string">'//*'</span>).extract() <span class="comment"># 所有元素</span></span><br><span class="line">response.xpath(<span class="string">'//title[@*]'</span>).extract() <span class="comment"># 所有带属性的 title 元素</span></span><br><span class="line">response.xpath(<span class="string">'//div/a | //div/p'</span>).extract()<span class="comment"># 所有 div 的 a 和 p</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//a/@href'</span>).extract_first()</span><br><span class="line">response.xpath(<span class="string">'a::attr(href)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复杂点的用法  href 属性包含 'image'</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "iamge")]/@href'</span>).extract()</span><br><span class="line">response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath css 可以嵌套</span></span><br><span class="line">response.xpath(<span class="string">'...'</span>).css(<span class="string">'...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># re</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "image")]/text()'</span>).re(<span class="string">r'Name:\s*(.*)'</span>)</span><br><span class="line"><span class="comment"># re re_first</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相对XPaths</span></span><br><span class="line"></span><br><span class="line">divs = response.xpath(<span class="string">'//div'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'//p'</span>):  <span class="comment"># this is wrong - gets all &lt;p&gt; from the whole document</span></span><br><span class="line"><span class="comment"># 下面是比较合适的处理方法(注意 .//p XPath的点前缀):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'.//p'</span>):  <span class="comment"># extracts all &lt;p&gt; inside</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> p.extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'p'</span>):</span><br></pre></td></tr></table></figure>



<h4 id="获取HTML-注释"><a href="#获取HTML-注释" class="headerlink" title="获取HTML 注释"></a>获取HTML 注释</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"> </span><br><span class="line">html_str = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;div id="box1"&gt;this from blog.csdn.net/lncxydjq , DO NOT COPY!</span></span><br><span class="line"><span class="string">  &lt;div id="box2"&gt;*****</span></span><br><span class="line"><span class="string">    &lt;!--can u get me, bitch?--&gt;</span></span><br><span class="line"><span class="string">  &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"> </span><br><span class="line">html = etree.HTML(html_str)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> html.xpath(<span class="string">'//div[@id="box1"]/div/node()'</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> type(html.xpath(<span class="string">'//div[@id="box1"]/div/node()'</span>)[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> html.xpath(<span class="string">'//div[@id="box1"]/div/node()'</span>)[<span class="number">1</span>].text</span><br><span class="line"> </span><br><span class="line"><span class="string">"""output:</span></span><br><span class="line"><span class="string">&lt;!--can u get me, bitch?--&gt;</span></span><br><span class="line"><span class="string">&lt;type 'lxml.etree._Comment'&gt;</span></span><br><span class="line"><span class="string">can u get me, bitch?</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>



<h3 id="spiders"><a href="#spiders" class="headerlink" title="spiders"></a>spiders</h3><h4 id="传递参数"><a href="#传递参数" class="headerlink" title="传递参数"></a>传递参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider -a category=electronics</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, category=None, *args, **kwargs)</span>:</span></span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category]</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<h4 id="Spider-类"><a href="#Spider-类" class="headerlink" title="Spider 类"></a>Spider 类</h4><p><strong>name</strong></p>
<p>唯一标识 spider</p>
<p><strong>allowed_domains</strong></p>
<p>包含 spider 允许爬取的域名列表</p>
<p><strong>start_urls</strong></p>
<p><strong>start_requests() ?</strong></p>
<p>必须返回可迭代对象</p>
<p>逻辑：</p>
<ol>
<li>未指定 start_urls: start_requests 生效</li>
<li>指定 start_urls: make_requests_from_url 生效</li>
</ol>
<p>可以使用 start_requests 在启动时以 post 登录某个网站：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">"http://www.example.com/login"</span>,</span><br><span class="line">                               formdata=&#123;<span class="string">'user'</span>: <span class="string">'john'</span>, <span class="string">'pass'</span>: <span class="string">'secret'</span>&#125;,</span><br><span class="line">                               callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logged_in</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">    <span class="comment"># each of them, with another callback</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><strong>make_requests_from_url(url) ?</strong></p>
<p>该方法接受一个URL并返回用于爬取的 Request 对象。 该方法在初始化request时被 start_requests() 调用，也被用于转化url为request。</p>
<p><strong>log(message[, level, component])</strong></p>
<p>log中自动带上该spider的 name 属性。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'A response from %s just arrived!'</span> % response.url)</span><br></pre></td></tr></table></figure>

<p><strong>parse 中返回多个 request 和 item</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> MyItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        sel = scrapy.Selector(response)</span><br><span class="line">        <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">'//h3'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> MyItem(title=h3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br></pre></td></tr></table></figure>



<h4 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h4><p>继承自 Spider，有一个新属性 rules 和一个可复写的方法 parse_start_url</p>
<p>Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)</p>
<p><strong>当编写爬虫规则时，请避免使用 parse 作为回调函数</strong>。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果 您覆盖了 parse 方法，crawl spider 将会运行失败。</p>
<p>Follow 默认为 False</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True)</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'Hi, this is an item page! %s'</span> % response.url)</span><br><span class="line"></span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</span><br><span class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>



<h4 id="XMLFeedSpider、CSVFeedSpider、SitemapSpider"><a href="#XMLFeedSpider、CSVFeedSpider、SitemapSpider" class="headerlink" title="XMLFeedSpider、CSVFeedSpider、SitemapSpider"></a>XMLFeedSpider、CSVFeedSpider、SitemapSpider</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">XMLFeedSpider例子</span><br><span class="line">该spider十分易用。下边是其中一个例子:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> XMLFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(XMLFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.xml'</span>]</span><br><span class="line">    iterator = <span class="string">'iternodes'</span> <span class="comment"># This is actually unnecessary, since it's the default value</span></span><br><span class="line">    itertag = <span class="string">'item'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_node</span><span class="params">(self, response, node)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a &lt;%s&gt; node!: %s'</span> % (self.itertag, <span class="string">''</span>.join(node.extract())))</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = node.xpath(<span class="string">'@id'</span>).extract()</span><br><span class="line">        item[<span class="string">'name'</span>] = node.xpath(<span class="string">'name'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = node.xpath(<span class="string">'description'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">CSVFeedSpider例子</span><br><span class="line">下面的例子和之前的例子很像，但使用了 CSVFeedSpider:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CSVFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CSVFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.csv'</span>]</span><br><span class="line">    delimiter = <span class="string">';'</span></span><br><span class="line">    headers = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'description'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_row</span><span class="params">(self, response, row)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a row!: %r'</span> % row)</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = row[<span class="string">'id'</span>]</span><br><span class="line">        item[<span class="string">'name'</span>] = row[<span class="string">'name'</span>]</span><br><span class="line">        item[<span class="string">'description'</span>] = row[<span class="string">'description'</span>]</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">SitemapSpider样例</span><br><span class="line">简单的例子: 使用 parse 处理通过sitemap发现的所有url:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.example.com/sitemap.xml'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># ... scrape item here ...</span></span><br></pre></td></tr></table></figure>



<h3 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h3><p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。以下是item pipeline的一些典型应用：</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h4 id="编写自己的-item-pipeline"><a href="#编写自己的-item-pipeline" class="headerlink" title="编写自己的 item pipeline"></a>编写自己的 item pipeline</h4><p><strong>process_item</strong></p>
<p>返回 item 或 抛出 DropItem 异常</p>
<p><strong>open_spider</strong></p>
<p>当spider被开启时，这个方法被调用。</p>
<p><strong>close_spider</strong><br>当spider被关闭时，这个方法被调用</p>
<h4 id="Item-pipeline-样例"><a href="#Item-pipeline-样例" class="headerlink" title="Item pipeline 样例"></a>Item pipeline 样例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证价格，同时丢弃没有价格的item</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br><span class="line">            </span><br><span class="line"><span class="comment"># 将item写入JSON文件</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">      </span><br><span class="line"><span class="comment"># 去重</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.ids_seen.add(item[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h4 id="代理ip"><a href="#代理ip" class="headerlink" title="代理ip"></a>代理ip</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span></span><br><span class="line">        self.logger.debug(<span class="string">'Get Exception'</span>)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = get_random_proxy() <span class="comment"># 形如 https://127.0.0.1:9743</span></span><br><span class="line">        <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure>



<p>settings.py 启动 item pipeline 插件</p>
<p>从小到大的顺序执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="Jobs-暂停，恢复爬虫"><a href="#Jobs-暂停，恢复爬虫" class="headerlink" title="Jobs: 暂停，恢复爬虫"></a>Jobs: 暂停，恢复爬虫</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启用或恢复一个爬虫，都是</span></span><br><span class="line">scrapy crawl douban -s JOBDIR=jobs/douban-1</span><br></pre></td></tr></table></figure>



<h3 id="爬虫优化"><a href="#爬虫优化" class="headerlink" title="爬虫优化"></a>爬虫优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加并发</span></span><br><span class="line">CONCURRENT_REQUESTS = <span class="number">100</span></span><br><span class="line"><span class="comment"># 降低log级别</span></span><br><span class="line"><span class="comment"># LOG_LEVEL = 'INFO'</span></span><br><span class="line"><span class="comment"># 禁止cookies</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 禁止重试</span></span><br><span class="line">RETRY_ENABLED = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 减小下载超时,有些网站就是慢</span></span><br><span class="line"><span class="comment"># DOWNLOAD_TIMEOUT = 15</span></span><br><span class="line"><span class="comment"># 禁止重定向</span></span><br><span class="line">REDIRECT_ENABLED = <span class="literal">False</span></span><br></pre></td></tr></table></figure>





<h2 id="课上教的"><a href="#课上教的" class="headerlink" title="课上教的"></a>课上教的</h2><h3 id="1-延迟获取"><a href="#1-延迟获取" class="headerlink" title="1 延迟获取"></a><strong>1</strong> 延迟获取</h3><h4 id="1-1-DOWNLOAD-DELAY"><a href="#1-1-DOWNLOAD-DELAY" class="headerlink" title="1.1 DOWNLOAD_DELAY"></a><strong>1.1</strong> DOWNLOAD_DELAY</h4><p>settings.py</p>
<p>设置DOWNLOAD_DELAY</p>
<p>数值越大，延迟越大。</p>
<h4 id="1-3-滚动加载"><a href="#1-3-滚动加载" class="headerlink" title="1.3 滚动加载"></a>1.3 滚动加载</h4><p>像那种页面滚动到下方，才新加载数据的网页，可以通过selenium执行脚本来实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先导入包</span></span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="comment"># 然后输入代码</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  spider.browser.get(request.url)</span><br><span class="line">  spider.browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line"><span class="keyword">except</span> TimeoutException <span class="keyword">as</span> e:</span><br><span class="line">  print(<span class="string">'超时'</span>)</span><br><span class="line">  spider.browser.execute_script(<span class="string">'window.stop()'</span>)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">return</span> HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding=<span class="string">"utf-8"</span>, request=request)</span><br></pre></td></tr></table></figure>

<p>滚动到页面底部的操作可以通过javascript代码’window.scrollTo(0, document.body.scrollHeight)’，来实现，所以要执行该脚本，方法是execute_script</p>
<h3 id="2-settings-py"><a href="#2-settings-py" class="headerlink" title="2 settings.py"></a><strong>2</strong> settings.py</h3><p>参考网址：<a href="https://www.cnblogs.com/longyunfeigu/p/9494408.html">scrapy的配置文件settings - 龙云飞谷 - 博客园</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==&gt;第一部分：基本配置&lt;===</span></span><br><span class="line"><span class="comment">#1、项目名称，默认的USER_AGENT由它来构成，也作为日志记录的日志名</span></span><br><span class="line">BOT_NAME = <span class="string">'Amazon'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬虫应用路径</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">'Amazon.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'Amazon.spiders'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、客户端User-Agent请求头</span></span><br><span class="line"><span class="comment">#USER_AGENT = 'Amazon (+http://www.yourdomain.com)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、是否遵循爬虫协议</span></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#5、是否支持cookie，cookiejar进行操作cookie，默认开启</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#6、Telnet用于查看当前爬虫的信息，操作爬虫等...使用telnet ip port ，然后通过命令操作</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_HOST = '127.0.0.1'</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_PORT = [6023,]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#7、Scrapy发送HTTP请求默认使用的请求头</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></span><br><span class="line"><span class="comment">#   'Accept-Language': 'en',</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第二部分：并发与延迟&lt;===</span></span><br><span class="line"><span class="comment">#1、下载器总共最大处理的并发请求数,默认值16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、每个域名能够被执行的最大并发请求数目，默认值8</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、能够被单个IP处理的并发请求数，默认值0，代表无限制，需要注意两点</span></span><br><span class="line"><span class="comment">#I、如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN将被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名</span></span><br><span class="line"><span class="comment">#II、该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、如果没有开启智能限速，这个值就代表一个规定死的值，代表对同一网址延迟请求的秒数</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第三部分：智能限速/自动节流：AutoThrottle extension&lt;===</span></span><br><span class="line"><span class="comment">#一：介绍</span></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.throttle <span class="keyword">import</span> AutoThrottle <span class="comment">#http://scrapy.readthedocs.io/en/latest/topics/autothrottle.html#topics-autothrottle</span></span><br><span class="line">设置目标：</span><br><span class="line"><span class="number">1</span>、比使用默认的下载延迟对站点更好</span><br><span class="line"><span class="number">2</span>、自动调整scrapy到最佳的爬取速度，所以用户无需自己调整下载延迟到最佳状态。用户只需要定义允许最大并发的请求，剩下的事情由该扩展组件自动完成</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#二：如何实现？</span></span><br><span class="line">在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。</span><br><span class="line">注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。 不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#三：限速算法</span></span><br><span class="line">自动限速算法基于以下规则调整下载延迟</span><br><span class="line"><span class="comment">#1、spiders开始时的下载延迟是基于AUTOTHROTTLE_START_DELAY的值</span></span><br><span class="line"><span class="comment">#2、当收到一个response，对目标站点的下载延迟=收到响应的延迟时间/AUTOTHROTTLE_TARGET_CONCURRENCY</span></span><br><span class="line"><span class="comment">#3、下一次请求的下载延迟就被设置成：对目标站点下载延迟时间和过去的下载延迟时间的平均值</span></span><br><span class="line"><span class="comment">#4、没有达到200个response则不允许降低延迟</span></span><br><span class="line"><span class="comment">#5、下载延迟不能变的比DOWNLOAD_DELAY更低或者比AUTOTHROTTLE_MAX_DELAY更高</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#四：配置使用</span></span><br><span class="line"><span class="comment">#开启True，默认False</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment">#起始的延迟</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment">#最小延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br><span class="line"><span class="comment">#最大延迟</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">10</span></span><br><span class="line"><span class="comment">#每秒并发请求数的平均值，不能高于 CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP，调高了则吞吐量增大强奸目标站点，调低了则对目标站点更加”礼貌“</span></span><br><span class="line"><span class="comment">#每个特定的时间点，scrapy并发请求的数目都可能高于或低于该值，这是爬虫视图达到的建议值而不是硬限制</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">16.0</span></span><br><span class="line"><span class="comment">#调试</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">True</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第四部分：爬取深度与爬取方式&lt;===</span></span><br><span class="line"><span class="comment">#1、爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span></span><br><span class="line"><span class="comment"># DEPTH_LIMIT = 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后进先出，深度优先</span></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 0</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'</span></span><br><span class="line"><span class="comment"># 先进先出，广度优先</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 1</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、调度器队列</span></span><br><span class="line"><span class="comment"># SCHEDULER = 'scrapy.core.scheduler.Scheduler'</span></span><br><span class="line"><span class="comment"># from scrapy.core.scheduler import Scheduler</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、访问URL去重</span></span><br><span class="line"><span class="comment"># DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第五部分：中间件、Pipelines、扩展&lt;===</span></span><br><span class="line"><span class="comment">#1、Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    'Amazon.middlewares.AmazonSpiderMiddleware': 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="comment"># 'Amazon.middlewares.DownMiddleware1': 543,</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、Configure item pipelines</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="comment"># 'Amazon.pipelines.CustomPipeline': 200,</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第六部分：缓存&lt;===</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">1. 启用缓存</span></span><br><span class="line"><span class="string">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import DummyPolicy</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 是否启用缓存策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_ENABLED = True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"</span></span><br><span class="line"><span class="comment"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存超时时间</span></span><br><span class="line"><span class="comment"># HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存保存路径</span></span><br><span class="line"><span class="comment"># HTTPCACHE_DIR = 'httpcache'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存忽略的Http状态码</span></span><br><span class="line"><span class="comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存存储的插件</span></span><br><span class="line"><span class="comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></span><br></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://juejin.im/post/5aec1bb9f265da0b9526f855">Scrapy框架的使用之Selector的用法 - 掘金</a></p>
<p>介绍了 css xpath re 的用法</p>
<h2 id="工具-–-Useful-packages"><a href="#工具-–-Useful-packages" class="headerlink" title="工具 – Useful packages"></a>工具 – Useful packages</h2><p><a href="https://github.com/further-reading/scrapy-gui">further-reading/scrapy-gui: A simple, Qt-Webengine powered web browser with built in functionality for basic scrapy webscraping support.</a></p>
<p>今天在豆瓣失败了，大多还是很好用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line"><span class="keyword">import</span> scrapy_gui</span><br><span class="line">scrapy_gui.open_browser()</span><br></pre></td></tr></table></figure>



<img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" />



<h3 id="gerapy"><a href="#gerapy" class="headerlink" title="gerapy"></a>gerapy</h3><p>主要用来管理本地或远程主机的 scrapy 项目，替代 scrapyd 的命令行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip3 install gerapy</span><br><span class="line">gerapy init</span><br><span class="line">gerapy init &lt;workspace&gt; # 留空 为 gerapy</span><br><span class="line">cd gerapy</span><br><span class="line">gerapy migrate</span><br><span class="line">gerapy createsuperuser</span><br><span class="line">gerapy runserver</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/Gerapy/Gerapy">Gerapy/Gerapy: Distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js</a><br><a href="https://ask.hellobi.com/blog/cuiqingcai/11195#articleHeader5">跟繁琐的命令行说拜拜！Gerapy分布式爬虫管理框架来袭！ - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a><br><a href="https://blog.csdn.net/baidu_32542573/article/details/79722390">python爬虫之Gerapy安装部署_Python_baidu_32542573的博客-CSDN博客</a></p>
<h2 id="防止封IP策略"><a href="#防止封IP策略" class="headerlink" title="防止封IP策略"></a>防止封IP策略</h2><p><a href="https://github.com/yidao620c/core-scrapy#">yidao620c/core-scrapy: python-scrapy demo</a></p>
<p>如果抓取太频繁了，就被被封IP</p>
<p>策略1：设置download_delay下载延迟，数字设置为5秒，越大越安全<br>策略2：禁止Cookie，某些网站会通过Cookie识别用户身份，禁用后使得服务器无法识别爬虫轨迹<br>策略3：使用user agent池。也就是每次发送的时候随机从池中选择不一样的浏览器头信息，防止暴露爬虫身份<br>策略4：使用IP池，这个需要大量的IP资源，貌似还达不到这个要求<br>策略5：分布式爬取，这个是针对大型爬虫系统的，对目前而言我们还用不到。</p>
<p>scrapy crawl shudan -s JOBDIR=jobs/shudan-1</p>
<h2 id="scrapy-cookbook"><a href="#scrapy-cookbook" class="headerlink" title="scrapy-cookbook"></a>scrapy-cookbook</h2><p>Contents:</p>
<ul>
<li>Scrapy教程01- 入门篇<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-01.html#scrapy">安装scrapy</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-01.html#">简单示例</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-01.html#scrapy">Scrapy特性一览</a></li>
</ul>
</li>
<li>Scrapy教程02- 完整示例<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#scrapy">创建Scrapy工程</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#item">定义我们的Item</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#spider">第一个Spider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#">运行爬虫</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#">处理链接</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#">导出抓取数据</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#">保存数据到数据库</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-02.html#">下一步</a></li>
</ul>
</li>
<li>Scrapy教程03- Spider详解<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-03.html#crawlspider">CrawlSpider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-03.html#xmlfeedspider">XMLFeedSpider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-03.html#csvfeedspider">CSVFeedSpider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-03.html#sitemapspider">SitemapSpider</a></li>
</ul>
</li>
<li>Scrapy教程04- Selector详解<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#">关于选择器</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#">使用选择器</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#">嵌套选择器</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#">使用正则表达式</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#xpath">XPath相对路径</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-04.html#xpath">XPath建议</a></li>
</ul>
</li>
<li>Scrapy教程05- Item详解<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item">定义Item</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item-fields">Item Fields</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item">Item使用示例</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item-loader">Item Loader</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#">输入/输出处理器</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item-loader">自定义Item Loader</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#field">在Field定义中声明输入/输出处理器</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#item-loader">Item Loader上下文</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-05.html#">内置的处理器</a></li>
</ul>
</li>
<li>Scrapy教程06- Item Pipeline<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-06.html#pipeline">编写自己的Pipeline</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-06.html#item-pipeline">Item Pipeline示例</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-06.html#item-pipeline">激活一个Item Pipeline组件</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-06.html#feed-exports">Feed exports</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-06.html#">请求和响应</a></li>
</ul>
</li>
<li>Scrapy教程07- 内置服务<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-07.html#email">发送email</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-07.html#spider">同一个进程运行多个Spider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-07.html#">分布式爬虫</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-07.html#">防止被封的策略</a></li>
</ul>
</li>
<li>Scrapy教程08- 文件与图片<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-08.html#files-pipeline">使用Files Pipeline</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-08.html#images-pipeline">使用Images Pipeline</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-08.html#">使用例子</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-08.html#">自定义媒体管道</a></li>
</ul>
</li>
<li>Scrapy教程09- 部署<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-09.html#scrapyd">部署到Scrapyd</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-09.html#scrapy-cloud">部署到Scrapy Cloud</a></li>
</ul>
</li>
<li>Scrapy教程10- 动态配置爬虫<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#scrapy">脚本运行Scrapy</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#spider">同一进程运行多个spider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#">定义规则表</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#item">定义文章Item</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#articlespider">定义ArticleSpider</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#pipeline">编写pipeline存储到数据库中</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-10.html#run-py">修改run.py启动脚本</a></li>
</ul>
</li>
<li>Scrapy教程11- 模拟登录<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-11.html#start-requests">重写start_requests方法</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-11.html#formrequest">使用FormRequest</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-11.html#requests-to-follow">重写_requests_to_follow</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-11.html#">页面处理方法</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-11.html#">完整源码</a></li>
</ul>
</li>
<li>Scrapy教程12- 抓取动态网站<ul>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#scrapy-splash">scrapy-splash简介</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#docker">安装docker</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#splash">安装Splash</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#scrapy-splash">安装scrapy-splash</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#scrapy-splash">配置scrapy-splash</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#scrapy-splash">使用scrapy-splash</a></li>
<li><a href="https://scrapy-cookbook.readthedocs.io/zh_CN/latest/scrapy-12.html#">使用实例</a></li>
</ul>
</li>
</ul>
</div><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html">scrapy</a></li><li><strong>本文作者：</strong><a href="https://zronghui.github.io">zronghui</a></li><li><strong>本文链接：</strong><a href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html">https://zronghui.github.io/研究生课程/信息系统实训/scrapy.html</a></li><li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># 推荐文章</span><br><span>  1.<a class="is-size-6" href="/Leetcode%20weekly%20contest/biweekly-contest-22.html" target="_blank">biweekly-contest-22</a><br></span><span>  2.<a class="is-size-6" href="/leetcode/%E7%A8%8B%E5%BA%8F%E5%91%98%E9%9D%A2%E8%AF%95%E9%87%91%E5%85%B8/%E9%9D%A2%E8%AF%95%E9%A2%98-01-07-%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5.html" target="_blank">面试题 01.07 旋转矩阵</a><br></span><span>  3.<a class="is-size-6" href="/Chrome/chrome-%E6%8F%92%E4%BB%B6.html" target="_blank">chrome 插件</a><br></span><span>  4.<a class="is-size-6" href="/Leetcode%20weekly%20contest/biweekly-contest-23.html" target="_blank">biweekly-contest-23</a><br></span><span>  5.<a class="is-size-6" href="/Leetcode%20weekly%20contest/weekly-contest-180.html" target="_blank">weekly-contest-180</a><br></span></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://i.loli.net/2020/03/05/MeFCknt1wipKvRA.png" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://i.loli.net/2020/03/05/MVJrxG3gO6qHSiz.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Mac/07-alfred.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">alfred</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC2%E6%AC%A1%E4%BD%9C%E4%B8%9A.html"><span class="level-item">信息系统实训 第二次作业</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.6.2/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '865cead3837eb7e055c28ed7c9679381',
            repo: 'blog_comment',
            owner: 'zronghui',
            clientID: 'f1c7f9da01111d2e7163',
            clientSecret: '3aaaf709bd8edda1f4c57484cceba5fad5218978',
            admin: ["zronghui"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-helloScrapy" href="#helloScrapy"><span>helloScrapy</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-settings-py" href="#settings-py"><span>settings.py</span></a></li><li><a class="is-flex" id="toc-item-pipelines-py" href="#pipelines-py"><span>pipelines.py</span></a></li><li><a class="is-flex" id="toc-item-items-py" href="#items-py"><span>items.py</span></a></li><li><a class="is-flex" id="toc-item-Spider-volmoe-py" href="#Spider-volmoe-py"><span>Spider&amp;#x2F;volmoe.py</span></a></li><li><a class="is-flex" id="toc-item-注意" href="#注意"><span>注意</span></a></li></ul></li><li><a class="is-flex" id="toc-item-学习" href="#学习"><span>学习</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-启动所有爬虫-代码启动" href="#启动所有爬虫-代码启动"><span>启动所有爬虫 代码启动</span></a></li><li><a class="is-flex" id="toc-item-启动一个爬虫" href="#启动一个爬虫"><span>启动一个爬虫</span></a></li><li><a class="is-flex" id="toc-item-事例-spider" href="#事例-spider"><span>事例 spider</span></a></li><li><a class="is-flex" id="toc-item-保存爬取到的数据" href="#保存爬取到的数据"><span>保存爬取到的数据</span></a></li><li><a class="is-flex" id="toc-item-在Shell中尝试Selector选择器" href="#在Shell中尝试Selector选择器"><span>在Shell中尝试Selector选择器</span></a></li><li><a class="is-flex" id="toc-item-命令行工具" href="#命令行工具"><span>命令行工具</span></a></li><li><a class="is-flex" id="toc-item-items" href="#items"><span>items</span></a></li><li><a class="is-flex" id="toc-item-选择器-css-xpath" href="#选择器-css-xpath"><span>选择器  css xpath</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-获取HTML-注释" href="#获取HTML-注释"><span>获取HTML 注释</span></a></li></ul></li><li><a class="is-flex" id="toc-item-spiders" href="#spiders"><span>spiders</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-传递参数" href="#传递参数"><span>传递参数</span></a></li><li><a class="is-flex" id="toc-item-Spider-类" href="#Spider-类"><span>Spider 类</span></a></li><li><a class="is-flex" id="toc-item-CrawlSpider" href="#CrawlSpider"><span>CrawlSpider</span></a></li><li><a class="is-flex" id="toc-item-XMLFeedSpider、CSVFeedSpider、SitemapSpider" href="#XMLFeedSpider、CSVFeedSpider、SitemapSpider"><span>XMLFeedSpider、CSVFeedSpider、SitemapSpider</span></a></li></ul></li><li><a class="is-flex" id="toc-item-Item-Pipeline" href="#Item-Pipeline"><span>Item Pipeline</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-编写自己的-item-pipeline" href="#编写自己的-item-pipeline"><span>编写自己的 item pipeline</span></a></li><li><a class="is-flex" id="toc-item-Item-pipeline-样例" href="#Item-pipeline-样例"><span>Item pipeline 样例</span></a></li><li><a class="is-flex" id="toc-item-代理ip" href="#代理ip"><span>代理ip</span></a></li></ul></li><li><a class="is-flex" id="toc-item-Jobs-暂停，恢复爬虫" href="#Jobs-暂停，恢复爬虫"><span>Jobs: 暂停，恢复爬虫</span></a></li><li><a class="is-flex" id="toc-item-爬虫优化" href="#爬虫优化"><span>爬虫优化</span></a></li></ul></li><li><a class="is-flex" id="toc-item-课上教的" href="#课上教的"><span>课上教的</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-延迟获取" href="#1-延迟获取"><span>1 延迟获取</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-1-DOWNLOAD-DELAY" href="#1-1-DOWNLOAD-DELAY"><span>1.1 DOWNLOAD_DELAY</span></a></li><li><a class="is-flex" id="toc-item-1-3-滚动加载" href="#1-3-滚动加载"><span>1.3 滚动加载</span></a></li></ul></li><li><a class="is-flex" id="toc-item-2-settings-py" href="#2-settings-py"><span>2 settings.py</span></a></li></ul></li><li><a class="is-flex" id="toc-item-参考资料" href="#参考资料"><span>参考资料</span></a></li><li><a class="is-flex" id="toc-item-工具-–-Useful-packages" href="#工具-–-Useful-packages"><span>工具 – Useful packages</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-gerapy" href="#gerapy"><span>gerapy</span></a></li></ul></li><li><a class="is-flex" id="toc-item-防止封IP策略" href="#防止封IP策略"><span>防止封IP策略</span></a></li><li><a class="is-flex" id="toc-item-scrapy-cookbook" href="#scrapy-cookbook"><span>scrapy-cookbook</span></a></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg" alt="zronghui"></figure><p class="title is-size-4 is-block line-height-inherit">zronghui</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">582</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">49</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">48</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zronghui" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zronghui"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-01T14:15:50.000Z">2020-12-01</time></p><p class="title is-6"><a class="link-muted" href="/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8AMySQL8-cookbook%E3%80%8B-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html">《MySQL8 cookbook》 阅读笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-11-29T14:16:28.000Z">2020-11-29</time></p><p class="title is-6"><a class="link-muted" href="/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8ARedis-%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B.html">《Redis 设计与实现》</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-11-29T14:16:13.000Z">2020-11-29</time></p><p class="title is-6"><a class="link-muted" href="/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8AMongoDB-%E5%AE%9E%E6%88%98%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html">《MongoDB 实战》阅读笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-11-29T14:15:56.000Z">2020-11-29</time></p><p class="title is-6"><a class="link-muted" href="/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8ALinux-Shell-%E8%84%9A%E6%9C%AC%E6%94%BB%E7%95%A5%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html">《Linux Shell 脚本攻略》阅读笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-11-29T07:52:12.000Z">2020-11-29</time></p><p class="title is-6"><a class="link-muted" href="/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4-MYSQL45%E8%AE%B2-%E7%AC%94%E8%AE%B0.html">极客时间-MYSQL45讲-笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Chrome/"><span class="level-start"><span class="level-item">Chrome</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Leetcode-weekly-contest/"><span class="level-start"><span class="level-item">Leetcode weekly contest</span></span><span class="level-end"><span class="level-item tag">41</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Mac/"><span class="level-start"><span class="level-item">Mac</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/blockchain/"><span class="level-start"><span class="level-item">blockchain</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/docker-k8s/"><span class="level-start"><span class="level-item">docker k8s</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/frontEnd/"><span class="level-start"><span class="level-item">frontEnd</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/go/"><span class="level-start"><span class="level-item">go</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/"><span class="level-start"><span class="level-item">java</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/java/Collections/"><span class="level-start"><span class="level-item">Collections</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag is-grey-lightest">35</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tree/"><span class="tag">Tree</span><span class="tag is-grey-lightest">34</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-Programming/"><span class="tag">Dynamic Programming</span><span class="tag is-grey-lightest">21</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Search/"><span class="tag">Binary Search</span><span class="tag is-grey-lightest">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hash-Table/"><span class="tag">Hash Table</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Depth-first-Search/"><span class="tag">Depth-first Search</span><span class="tag is-grey-lightest">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Two-Pointers/"><span class="tag">Two Pointers</span><span class="tag is-grey-lightest">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag is-grey-lightest">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linked-List/"><span class="tag">Linked List</span><span class="tag is-grey-lightest">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stack/"><span class="tag">Stack</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backtracking/"><span class="tag">Backtracking</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Heap/"><span class="tag">Heap</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Breadth-first-Search/"><span class="tag">Breadth-first Search</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tricks/"><span class="tag">tricks</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Design/"><span class="tag">Design</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bit-Manipulation/"><span class="tag">Bit Manipulation</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sort/"><span class="tag">Sort</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Divide-and-Conquer/"><span class="tag">Divide and Conquer</span><span class="tag is-grey-lightest">5</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=zronghui&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="zronghui" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div><p class="help">输入邮箱开始订阅，更博后邮件通知！</p></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://www.wntool.com/logo/image.php?output=png?fsize=35&amp;font=Snow.ttf&amp;text=zronghui&amp;mirror=No&amp;color=9933FF&amp;vcolor=3333FF&amp;bgcolor=FFFFFF&amp;alpha=yes&amp;output=png&amp;spacing=5&amp;shadow=no&amp;transparent=yes&amp;icon=no&amp;iconic=&amp;top_spacing=5&amp;left_spacing=6&amp;icon_size=48" alt="zronghui的博客" height="28"></a><p class="size-small"><span>&copy; 2020 zronghui</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>,Modify by <a href="https://github.com/removeif" target="_blank">removeif</a> <br>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br>    方便于网友与自己学习交流，如有侵权，请<a href="https://zronghui.github.io/message/" target="_blank">留言</a>，立即处理]<br><script type="text/javascript" src="/js/statistics.js"></script><span id="statistic-times"></span><br></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zronghui"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-Hans");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://zronghui.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>