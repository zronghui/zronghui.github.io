<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>scrapy - zronghui的博客</title><meta description="[TOC]"><meta property="og:type" content="blog"><meta property="og:title" content="zronghui"><meta property="og:url" content="https://zronghui.github.io/"><meta property="og:site_name" content="zronghui"><meta property="og:description" content="[TOC]"><meta property="og:image" content="https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png# https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg"><meta property="article:published_time" content="2020-03-10T03:26:04.000Z"><meta property="article:modified_time" content="2020-03-12T14:24:26.000Z"><meta property="article:author" content="removeif"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png# https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zronghui.github.io/"},"headline":"zronghui","image":["https://i.loli.net/2020/03/01/Lxsrj19ucvzetiM.png#https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg# https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@latest/img/avatar.jpg"],"datePublished":"2020-03-10T03:26:04.000Z","dateModified":"2020-03-12T14:24:26.000Z","author":{"@type":"Person","name":"removeif"},"description":"[TOC]"}</script><link rel="alternative" href="/atom.xml" title="zronghui的博客" type="application/atom+xml"><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@latest/img/wico.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://www.wntool.com/logo/image.php?output=png?fsize=35&amp;font=Snow.ttf&amp;text=zronghui&amp;mirror=No&amp;color=9933FF&amp;vcolor=3333FF&amp;bgcolor=FFFFFF&amp;alpha=yes&amp;output=png&amp;spacing=5&amp;shadow=no&amp;transparent=yes&amp;icon=no&amp;iconic=&amp;top_spacing=5&amp;left_spacing=6&amp;icon_size=48" alt="zronghui的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/media">影音</a><a class="navbar-item" href="/album">相册</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zronghui"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-03-10T03:26:04.000Z">2020-03-10</time><a class="commentCountImg" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html#comment-container"><span class="display-none-class">865cead3837eb7e055c28ed7c9679381</span><img class="not-gallery-item" src="/img/chat.svg"> <span class="commentCount" id="865cead3837eb7e055c28ed7c9679381"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a><span> / </span><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/">信息系统实训</a></span><span class="level-item">24 minutes 读完 (大约 3610 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">scrapy</h1><div class="content"><p>[TOC]</p>
<a id="more"></a>

<p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html">Scrapy入门教程 — Scrapy 0.24.6 文档</a></p>
<h2 id="helloScrapy"><a href="#helloScrapy" class="headerlink" title="helloScrapy"></a>helloScrapy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line">scrapy startproject helloScrapy</span><br><span class="line">scrapy genspider volmoe volmoe.com</span><br><span class="line">scrapy genspider douban douban.com</span><br></pre></td></tr></table></figure>



<h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><p>300 是权重，决定多个 pipeline 执行的顺序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ROBOTSTXT_OBEY = True</span></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'helloScrapy.pipelines.HelloscrapyPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理爬虫所爬取到的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file = open(<span class="string">'books.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 现将item数据转为字典类型，再将其保存为json文件</span></span><br><span class="line">        text = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">'\n'</span></span><br><span class="line">        <span class="comment"># 写入本地</span></span><br><span class="line">        self.file.write(text)</span><br><span class="line">        <span class="comment"># 会将item打印到屏幕上，方便观察</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫关闭时所执行的操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>



<h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    book_name = scrapy.Field()</span><br><span class="line">    book_url = scrapy.Field()</span><br><span class="line">    book_desc = scrapy.Field()</span><br></pre></td></tr></table></figure>



<h3 id="Spider-volmoe-py"><a href="#Spider-volmoe-py" class="headerlink" title="Spider/volmoe.py"></a>Spider/volmoe.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> zhconv <span class="keyword">import</span> convert</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helloScrapy.items <span class="keyword">import</span> BookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolmoeSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'volmoe'</span></span><br><span class="line">    allowed_domains = [<span class="string">'volmoe.com'</span>]</span><br><span class="line">    <span class="comment"># start_urls = [f'https://volmoe.com/l/all,all,all,sortpoint,all,all/&#123;i&#125;.htm' for i in range(1, 2)]</span></span><br><span class="line">    start_urls = [<span class="string">f'https://volmoe.com/l/all,all,all,sortpoint,all,all/<span class="subst">&#123;i&#125;</span>.htm'</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">473</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自动调用 parse ，解析 item URL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        itemUrlList = list(set(response.xpath(<span class="string">'/html/body/div[7]/table'</span>) \</span><br><span class="line">                               .xpath(<span class="string">'//a/@href'</span>) \</span><br><span class="line">                               .re(<span class="string">r'https://volmoe.com/c/\d+.htm'</span>)))</span><br><span class="line">        <span class="keyword">for</span> itemUrl <span class="keyword">in</span> itemUrlList:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=itemUrl, callback=self.parse_item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 手动调用 parse_item ，解析 item</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            item = BookItem()</span><br><span class="line">            item[<span class="string">'book_url'</span>] = response.url</span><br><span class="line">            item[<span class="string">'book_name'</span>] = convert(response.xpath(<span class="string">'//div/b/text()'</span>).extract()[<span class="number">0</span>], <span class="string">'zh-cn'</span>)</span><br><span class="line">            item[<span class="string">'book_desc'</span>] = convert(response.xpath(<span class="string">'//*[@id="desc_text"]/text()'</span>).extract()[<span class="number">0</span>].strip(), <span class="string">'zh-cn'</span>)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure>

<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol>
<li><p>爬取 列表页+详情页 用 rules 更方便，但是我用了，没有生效，所以手动指定函数处理</p>
</li>
<li><p>直接在网页上的 xpath 可能会与爬虫请求到的结构不同，所以可以用 scrapy shell “url”， view(response), 在网页上使用 xpath finder , xpath helper 查看指定元素的 xpath</p>
</li>
<li><p>scrapy genspider mydomain mydomain.com，最后的 mydomain.com 不用加 http 和 com 后面的/, 如 <del><a href="http://mydomain.com/">http://mydomain.com/</a></del></p>
</li>
<li><p>借助 <a href="https://github.com/further-reading/scrapy-gui">https://github.com/further-reading/scrapy-gui</a> 测试 CSS <del>xpath</del></p>
<img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" />

<ol start="5">
<li><p>获得的 URL 没有域名时：url = response.urljoin(next)  ?</p>
</li>
<li><p>可以在一个爬虫中用 custom_settings <img src="https://i.loli.net/2020/03/12/YLS5ftcphlDMTzx.png" alt="YLS5ftcphlDMTzx" style="zoom: 25%;" /></p>
</li>
<li><p>scrapy对request的URL去重 通过 yield scrapy.Request(url, self.parse, dont_filter=False) 里的 dont_filter（默认False） 实现</p>
</li>
<li><p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数运行爬虫，如：scrapy crawl spider1 -L WARNING</p>
</li>
<li><p>scrapy shell 设置 UA</p>
<img src="https://i.loli.net/2020/03/12/H84sqghj3rNXpWv.jpg" alt="H84sqghj3rNXpWv" style="zoom:50%;" />

</li>
</ol>
</li>
</ol>
<h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h3 id="启动所有爬虫？？"><a href="#启动所有爬虫？？" class="headerlink" title="启动所有爬虫？？"></a>启动所有爬虫？？</h3><h3 id="启动一个爬虫"><a href="#启动一个爬虫" class="headerlink" title="启动一个爬虫"></a>启动一个爬虫</h3><p>scrapy crawl volmoe</p>
<h3 id="事例-spider"><a href="#事例-spider" class="headerlink" title="事例 spider"></a>事例 spider</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</span><br><span class="line">            item = DmozItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'text()'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>



<h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>最简单存储爬取的数据的方式是使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/feed-exports.html#topics-feed-exports">Feed exports</a>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dmoz -o items.json</span><br></pre></td></tr></table></figure>

<p>该命令将采用 <a href="http://en.wikipedia.org/wiki/JSON">JSON</a> 格式对爬取的数据进行序列化，生成 <code>items.json</code> 文件。</p>
<p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的item做更多更为复杂的操作，您可以编写 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/item-pipeline.html#topics-item-pipeline">Item Pipeline</a> 。 </p>
<h3 id="在Shell中尝试Selector选择器"><a href="#在Shell中尝试Selector选择器" class="headerlink" title="在Shell中尝试Selector选择器"></a>在Shell中尝试Selector选择器</h3><p>为了介绍Selector的使用方法，接下来我们将要使用内置的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html#topics-shell">Scrapy shell</a> 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p>
<p>您需要进入项目的根目录，执行下列命令来启动shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&quot;</span><br></pre></td></tr></table></figure>

<p>注解</p>
<p>当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 <code>&amp;</code> 字符)会导致Scrapy运行失败。</p>
<p>shell的输出类似:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[ ... Scrapy log here ... ]</span><br><span class="line"></span><br><span class="line">2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;</span><br><span class="line">[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span><br><span class="line">[s]   view(response)    View response in a browser</span><br><span class="line"></span><br><span class="line">In [1]:</span><br></pre></td></tr></table></figure>

<p>当shell载入后，您将得到一个包含response数据的本地 <code>response</code> 变量。输入 <code>response.body</code> 将输出response的包体， 输出 <code>response.headers</code> 可以看到response的包头。</p>
<p>更为重要的是，当输入 <code>response.selector</code> 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 <code>response.selector.xpath()</code> 、 <code>response.selector.css()</code> 的 快捷方法(shortcut): <code>response.xpath()</code> 和 <code>response.css()</code> 。</p>
<p>同时，shell根据response提前初始化了变量 <code>sel</code> 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。</p>
<p>让我们来试试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: response.xpath(<span class="string">'//title'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [&lt;Selector xpath=<span class="string">'//title'</span> data=<span class="string">u'&lt;title&gt;Open Directory - Computers: Progr'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: response.xpath(<span class="string">'//title'</span>).extract()</span><br><span class="line">Out[<span class="number">2</span>]: [<span class="string">u'&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: response.xpath(<span class="string">'//title/text()'</span>)</span><br><span class="line">Out[<span class="number">3</span>]: [&lt;Selector xpath=<span class="string">'//title/text()'</span> data=<span class="string">u'Open Directory - Computers: Programming:'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: response.xpath(<span class="string">'//title/text()'</span>).extract()</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="string">u'Open Directory - Computers: Programming: Languages: Python: Books'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: response.xpath(<span class="string">'//title/text()'</span>).re(<span class="string">'(\w+):'</span>)</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="string">u'Computers'</span>, <span class="string">u'Programming'</span>, <span class="string">u'Languages'</span>, <span class="string">u'Python'</span>]</span><br></pre></td></tr></table></figure>

<h3 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h3><p>详见：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html">命令行工具(Command line tools) — Scrapy 0.24.6 文档</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以指定模板（怎么自定义模板？）</span></span><br><span class="line">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看 basic 模板</span></span><br><span class="line">scrapy genspider -d basic</span><br><span class="line"></span><br><span class="line">scrapy crawl myspider</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行contract检查, 运行没问题就好</span></span><br><span class="line">scrapy check [-l] &lt;spider&gt;</span><br><span class="line"></span><br><span class="line">scrapy list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 EDITOR 中设定的编辑器编辑给定的spider</span></span><br><span class="line">scrapy edit spider1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出（命令行？）</span></span><br><span class="line">scrapy fetch &lt;url&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ** 在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。</span></span><br><span class="line">scrapy view &lt;url&gt;</span><br><span class="line"></span><br><span class="line">scrapy shell [url]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。</span></span><br><span class="line">scrapy parse &lt;url&gt; [options]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取Scrapy的设定</span></span><br><span class="line">scrapy settings [options]</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get BOT_NAME</span></span><br><span class="line">scrapybot</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get DOWNLOAD_DELAY</span></span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在未创建项目的情况下，运行一个编写在Python文件中的spider。</span></span><br><span class="line">scrapy runspider &lt;spider_file.py&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy runspider myspider.py</span></span><br><span class="line">[ ... spider starts crawling ... ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。</span></span><br><span class="line">scrapy version [-v]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将项目部署到Scrapyd服务？</span></span><br><span class="line">scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行benchmark测试</span></span><br><span class="line">scrapy bench</span><br></pre></td></tr></table></figure>

<h3 id="items"><a href="#items" class="headerlink" title="items"></a>items</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个 item</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># item API和 dict API 非常相似</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一、与Item配合</span></span><br><span class="line"><span class="comment"># 创建item</span></span><br><span class="line">Product(name=<span class="string">'PC'</span>, price=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 获取字段的值</span></span><br><span class="line">p.get(<span class="string">'name'</span>)</span><br><span class="line">p.get(<span class="string">'name'</span>, <span class="string">'default'</span>)</span><br><span class="line">p[<span class="string">'name'</span>]</span><br><span class="line"><span class="string">'name'</span> <span class="keyword">in</span> product <span class="comment"># True  name 字段是否有填充</span></span><br><span class="line"><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields <span class="comment"># False  last_updated 是否是声明的字段</span></span><br><span class="line"><span class="comment"># 设置字段的值</span></span><br><span class="line">p[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></span><br><span class="line"><span class="comment"># 获取所有获取到的值</span></span><br><span class="line">p.keys() <span class="comment"># ['price', 'name']</span></span><br><span class="line">p.items() <span class="comment"># [('price', 1000), ('name', 'Desktop PC')]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他任务</span></span><br><span class="line"><span class="comment">#   复制item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product2 = Product(product)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product3 = product2.copy()</span><br><span class="line"><span class="comment">#   根据item创建字典(dict):</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dict(product) <span class="comment"># create a dict from all populated values</span></span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="number">1000</span>, <span class="string">'name'</span>: <span class="string">'Desktop PC'</span>&#125;</span><br><span class="line"><span class="comment">#   根据字典(dict)创建item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Product(&#123;<span class="string">'name'</span>: <span class="string">'Laptop PC'</span>, <span class="string">'price'</span>: <span class="number">1500</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩展Item</span></span><br><span class="line"><span class="comment">#   可以通过继承原始的Item来扩展item(添加更多的字段或者修改某些字段的元数据)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscountedProduct</span><span class="params">(Product)</span>:</span></span><br><span class="line">    discount_percent = scrapy.Field(serializer=str)</span><br><span class="line">    discount_expiration_date = scrapy.Field()</span><br><span class="line"><span class="comment"># Item对象</span></span><br><span class="line"><span class="comment"># 字段(Field)对象</span></span><br></pre></td></tr></table></figure>



<h3 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//div/a/text()'</span>).extract()</span><br><span class="line">response.css(<span class="string">'div a::text'</span>).extract()</span><br><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//a/@href'</span>).extract_first()</span><br><span class="line">response.xpath(<span class="string">'a::attr(href)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复杂点的用法  href 属性包含 'image'</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "iamge")]/@href'</span>).extract()</span><br><span class="line">response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath css 可以嵌套</span></span><br><span class="line">response.xpath(<span class="string">'...'</span>).css(<span class="string">'...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># re</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "image")]/text()'</span>).re(<span class="string">r'Name:\s*(.*)'</span>)</span><br><span class="line"><span class="comment"># re re_first</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相对XPaths</span></span><br><span class="line"></span><br><span class="line">divs = response.xpath(<span class="string">'//div'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'//p'</span>):  <span class="comment"># this is wrong - gets all &lt;p&gt; from the whole document</span></span><br><span class="line"><span class="comment"># 下面是比较合适的处理方法(注意 .//p XPath的点前缀):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'.//p'</span>):  <span class="comment"># extracts all &lt;p&gt; inside</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> p.extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'p'</span>):</span><br></pre></td></tr></table></figure>

<h3 id="spiders"><a href="#spiders" class="headerlink" title="spiders"></a>spiders</h3><h4 id="传递参数"><a href="#传递参数" class="headerlink" title="传递参数"></a>传递参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider -a category=electronics</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, category=None, *args, **kwargs)</span>:</span></span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category]</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<h4 id="Spider-类"><a href="#Spider-类" class="headerlink" title="Spider 类"></a>Spider 类</h4><p><strong>name</strong></p>
<p>唯一标识 spider</p>
<p><strong>allowed_domains</strong></p>
<p>包含 spider 允许爬取的域名列表</p>
<p><strong>start_urls</strong></p>
<p><strong>start_requests() ?</strong></p>
<p>必须返回可迭代对象</p>
<p>逻辑：</p>
<ol>
<li>未指定 start_urls: start_requests 生效</li>
<li>指定 start_urls: make_requests_from_url 生效</li>
</ol>
<p>可以使用 start_requests 在启动时以 post 登录某个网站：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">"http://www.example.com/login"</span>,</span><br><span class="line">                               formdata=&#123;<span class="string">'user'</span>: <span class="string">'john'</span>, <span class="string">'pass'</span>: <span class="string">'secret'</span>&#125;,</span><br><span class="line">                               callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logged_in</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">    <span class="comment"># each of them, with another callback</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><strong>make_requests_from_url(url) ?</strong></p>
<p>该方法接受一个URL并返回用于爬取的 Request 对象。 该方法在初始化request时被 start_requests() 调用，也被用于转化url为request。</p>
<p><strong>log(message[, level, component])</strong></p>
<p>log中自动带上该spider的 name 属性。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'A response from %s just arrived!'</span> % response.url)</span><br></pre></td></tr></table></figure>

<p><strong>parse 中返回多个 request 和 item</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> MyItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        sel = scrapy.Selector(response)</span><br><span class="line">        <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">'//h3'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> MyItem(title=h3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br></pre></td></tr></table></figure>



<h4 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h4><p>继承自 Spider，有一个新属性 rules 和一个可复写的方法 parse_start_url</p>
<p>Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)</p>
<p><strong>当编写爬虫规则时，请避免使用 parse 作为回调函数</strong>。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果 您覆盖了 parse 方法，crawl spider 将会运行失败。</p>
<p>Follow 默认为 False</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True)</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'Hi, this is an item page! %s'</span> % response.url)</span><br><span class="line"></span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</span><br><span class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>



<h4 id="XMLFeedSpider、CSVFeedSpider、SitemapSpider"><a href="#XMLFeedSpider、CSVFeedSpider、SitemapSpider" class="headerlink" title="XMLFeedSpider、CSVFeedSpider、SitemapSpider"></a>XMLFeedSpider、CSVFeedSpider、SitemapSpider</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">XMLFeedSpider例子</span><br><span class="line">该spider十分易用。下边是其中一个例子:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> XMLFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(XMLFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.xml'</span>]</span><br><span class="line">    iterator = <span class="string">'iternodes'</span> <span class="comment"># This is actually unnecessary, since it's the default value</span></span><br><span class="line">    itertag = <span class="string">'item'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_node</span><span class="params">(self, response, node)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a &lt;%s&gt; node!: %s'</span> % (self.itertag, <span class="string">''</span>.join(node.extract())))</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = node.xpath(<span class="string">'@id'</span>).extract()</span><br><span class="line">        item[<span class="string">'name'</span>] = node.xpath(<span class="string">'name'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = node.xpath(<span class="string">'description'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">CSVFeedSpider例子</span><br><span class="line">下面的例子和之前的例子很像，但使用了 CSVFeedSpider:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CSVFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CSVFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.csv'</span>]</span><br><span class="line">    delimiter = <span class="string">';'</span></span><br><span class="line">    headers = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'description'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_row</span><span class="params">(self, response, row)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a row!: %r'</span> % row)</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = row[<span class="string">'id'</span>]</span><br><span class="line">        item[<span class="string">'name'</span>] = row[<span class="string">'name'</span>]</span><br><span class="line">        item[<span class="string">'description'</span>] = row[<span class="string">'description'</span>]</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">SitemapSpider样例</span><br><span class="line">简单的例子: 使用 parse 处理通过sitemap发现的所有url:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.example.com/sitemap.xml'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># ... scrape item here ...</span></span><br></pre></td></tr></table></figure>



<h3 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h3><p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。以下是item pipeline的一些典型应用：</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h4 id="编写自己的-item-pipeline"><a href="#编写自己的-item-pipeline" class="headerlink" title="编写自己的 item pipeline"></a>编写自己的 item pipeline</h4><p><strong>process_item</strong></p>
<p>返回 item 或 抛出 DropItem 异常</p>
<p><strong>open_spider</strong></p>
<p>当spider被开启时，这个方法被调用。</p>
<p><strong>close_spider</strong><br>当spider被关闭时，这个方法被调用</p>
<h4 id="Item-pipeline-样例"><a href="#Item-pipeline-样例" class="headerlink" title="Item pipeline 样例"></a>Item pipeline 样例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证价格，同时丢弃没有价格的item</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br><span class="line">            </span><br><span class="line"><span class="comment"># 将item写入JSON文件</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">      </span><br><span class="line"><span class="comment"># 去重</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.ids_seen.add(item[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h4 id="代理ip"><a href="#代理ip" class="headerlink" title="代理ip"></a>代理ip</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span></span><br><span class="line">        self.logger.debug(<span class="string">'Get Exception'</span>)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = get_random_proxy() <span class="comment"># 形如 https://127.0.0.1:9743</span></span><br><span class="line">        <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure>



<p>settings.py 启动 item pipeline 插件</p>
<p>从小到大的顺序执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h2 id="Jobs-暂停，恢复爬虫"><a href="#Jobs-暂停，恢复爬虫" class="headerlink" title="Jobs: 暂停，恢复爬虫"></a>Jobs: 暂停，恢复爬虫</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启用或恢复一个爬虫，都是</span></span><br><span class="line">scrapy crawl douban -s JOBDIR=jobs/douban-1</span><br></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://juejin.im/post/5aec1bb9f265da0b9526f855">Scrapy框架的使用之Selector的用法 - 掘金</a></p>
<p>介绍了 css xpath re 的用法</p>
<h2 id="工具-–-Useful-packages"><a href="#工具-–-Useful-packages" class="headerlink" title="工具 – Useful packages"></a>工具 – Useful packages</h2><p><a href="https://github.com/further-reading/scrapy-gui">further-reading/scrapy-gui: A simple, Qt-Webengine powered web browser with built in functionality for basic scrapy webscraping support.</a></p>
<p>今天在豆瓣失败了，大多还是很好用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line"><span class="keyword">import</span> scrapy_gui</span><br><span class="line">scrapy_gui.open_browser()</span><br></pre></td></tr></table></figure>



<img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" />



<h3 id="gerapy"><a href="#gerapy" class="headerlink" title="gerapy"></a>gerapy</h3><p>主要用来管理本地或远程主机的 scrapy 项目，替代 scrapyd 的命令行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip3 install gerapy</span><br><span class="line">gerapy init</span><br><span class="line">gerapy init &lt;workspace&gt; # 留空 为 gerapy</span><br><span class="line">cd gerapy</span><br><span class="line">gerapy migrate</span><br><span class="line">gerapy createsuperuser</span><br><span class="line">gerapy runserver</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/Gerapy/Gerapy">Gerapy/Gerapy: Distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js</a><br><a href="https://ask.hellobi.com/blog/cuiqingcai/11195#articleHeader5">跟繁琐的命令行说拜拜！Gerapy分布式爬虫管理框架来袭！ - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a><br><a href="https://blog.csdn.net/baidu_32542573/article/details/79722390">python爬虫之Gerapy安装部署_Python_baidu_32542573的博客-CSDN博客</a></p>
<p>scrapy crawl shudan -s JOBDIR=jobs/shudan-1</p>
</div><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html">scrapy</a></li><li><strong>本文作者：</strong><a href="https://zronghui.github.io">zronghui</a></li><li><strong>本文链接：</strong><a href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html">https://zronghui.github.io/研究生课程/信息系统实训/scrapy.html</a></li><li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</li></ul><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># 推荐文章</span><br><span>  1.<a class="is-size-6" href="/theme/%E5%8D%9A%E5%AE%A2%E6%BA%90%E7%A0%81%E5%88%86%E4%BA%AB.html" target="_blank">博客源码分享</a><br></span><span>  2.<a class="is-size-6" href="/theme/%E5%8D%9A%E5%AE%A2%E4%B8%ADgitalk%E6%9C%80%E6%96%B0%E8%AF%84%E8%AE%BA%E7%9A%84%E8%8E%B7%E5%8F%96.html" target="_blank">博客中gitalk最新评论的获取</a><br></span><span>  3.<a class="is-size-6" href="/theme/github-Issue-%E4%BD%9C%E4%B8%BA%E5%8D%9A%E5%AE%A2%E5%BE%AE%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%BA%94%E7%94%A8.html" target="_blank">github Issue 作为博客微型数据库的应用</a><br></span><span>  4.<a class="is-size-6" href="/theme/%E4%B8%8D%E8%92%9C%E5%AD%90%E7%BB%9F%E8%AE%A1%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.html" target="_blank">不蒜子统计常见问题</a><br></span><span>  5.<a class="is-size-6" href="/%E5%A4%A9%E4%B9%A6/CNCF-%C3%97-Alibaba%E4%BA%91%E5%8E%9F%E7%94%9F%E6%8A%80%E6%9C%AF%E5%85%AC%E5%BC%80%E8%AF%BE/%E7%AC%AC%201%202%E8%AE%B2.html" target="_blank">CNCF × Alibaba云原生技术公开课</a><br></span></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://i.loli.net/2020/03/05/MeFCknt1wipKvRA.png" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://i.loli.net/2020/03/05/MVJrxG3gO6qHSiz.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Mac/07-alfred.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">alfred</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC2%E6%AC%A1%E4%BD%9C%E4%B8%9A.html"><span class="level-item">信息系统实训 第二次作业</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"><script src="/js/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '865cead3837eb7e055c28ed7c9679381',
            repo: 'blog_comment',
            owner: 'zronghui',
            clientID: 'f1c7f9da01111d2e7163',
            clientSecret: '3aaaf709bd8edda1f4c57484cceba5fad5218978',
            admin: ["removeif"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-helloScrapy" href="#helloScrapy"><span>helloScrapy</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-settings-py" href="#settings-py"><span>settings.py</span></a></li><li><a class="is-flex" id="toc-item-pipelines-py" href="#pipelines-py"><span>pipelines.py</span></a></li><li><a class="is-flex" id="toc-item-items-py" href="#items-py"><span>items.py</span></a></li><li><a class="is-flex" id="toc-item-Spider-volmoe-py" href="#Spider-volmoe-py"><span>Spider&amp;#x2F;volmoe.py</span></a></li><li><a class="is-flex" id="toc-item-注意" href="#注意"><span>注意</span></a></li></ul></li><li><a class="is-flex" id="toc-item-学习" href="#学习"><span>学习</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-启动所有爬虫？？" href="#启动所有爬虫？？"><span>启动所有爬虫？？</span></a></li><li><a class="is-flex" id="toc-item-启动一个爬虫" href="#启动一个爬虫"><span>启动一个爬虫</span></a></li><li><a class="is-flex" id="toc-item-事例-spider" href="#事例-spider"><span>事例 spider</span></a></li><li><a class="is-flex" id="toc-item-保存爬取到的数据" href="#保存爬取到的数据"><span>保存爬取到的数据</span></a></li><li><a class="is-flex" id="toc-item-在Shell中尝试Selector选择器" href="#在Shell中尝试Selector选择器"><span>在Shell中尝试Selector选择器</span></a></li><li><a class="is-flex" id="toc-item-命令行工具" href="#命令行工具"><span>命令行工具</span></a></li><li><a class="is-flex" id="toc-item-items" href="#items"><span>items</span></a></li><li><a class="is-flex" id="toc-item-选择器" href="#选择器"><span>选择器</span></a></li><li><a class="is-flex" id="toc-item-spiders" href="#spiders"><span>spiders</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-传递参数" href="#传递参数"><span>传递参数</span></a></li><li><a class="is-flex" id="toc-item-Spider-类" href="#Spider-类"><span>Spider 类</span></a></li><li><a class="is-flex" id="toc-item-CrawlSpider" href="#CrawlSpider"><span>CrawlSpider</span></a></li><li><a class="is-flex" id="toc-item-XMLFeedSpider、CSVFeedSpider、SitemapSpider" href="#XMLFeedSpider、CSVFeedSpider、SitemapSpider"><span>XMLFeedSpider、CSVFeedSpider、SitemapSpider</span></a></li></ul></li><li><a class="is-flex" id="toc-item-Item-Pipeline" href="#Item-Pipeline"><span>Item Pipeline</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-编写自己的-item-pipeline" href="#编写自己的-item-pipeline"><span>编写自己的 item pipeline</span></a></li><li><a class="is-flex" id="toc-item-Item-pipeline-样例" href="#Item-pipeline-样例"><span>Item pipeline 样例</span></a></li><li><a class="is-flex" id="toc-item-代理ip" href="#代理ip"><span>代理ip</span></a></li></ul></li></ul></li><li><a class="is-flex" id="toc-item-Jobs-暂停，恢复爬虫" href="#Jobs-暂停，恢复爬虫"><span>Jobs: 暂停，恢复爬虫</span></a></li><li><a class="is-flex" id="toc-item-参考资料" href="#参考资料"><span>参考资料</span></a></li><li><a class="is-flex" id="toc-item-工具-–-Useful-packages" href="#工具-–-Useful-packages"><span>工具 – Useful packages</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-gerapy" href="#gerapy"><span>gerapy</span></a></li></ul></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="https://i.loli.net/2020/02/27/giKJ3UbFhv69o7T.jpg" alt="zronghui"></figure><p class="title is-size-4 is-block line-height-inherit">zronghui</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">226</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">28</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">47</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="/null" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zronghui"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-12T10:11:57.000Z">2020-03-12</time></p><p class="title is-6"><a class="link-muted" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC1%E6%AC%A1%E4%BD%9C%E4%B8%9A.html">信息系统实训 第1次作业</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a> / <a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/">信息系统实训</a> / <a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/">提交作业</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-12T04:20:43.000Z">2020-03-12</time></p><p class="title is-6"><a class="link-muted" href="/%E6%95%B0%E6%8D%AE%E5%BA%93/redis.html">redis</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-11T14:58:26.000Z">2020-03-11</time></p><p class="title is-6"><a class="link-muted" href="/frontEnd/%E5%89%8D%E7%AB%AF%E5%B7%A5%E5%85%B7.html">前端工具</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/frontEnd/">frontEnd</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-11T08:16:25.000Z">2020-03-11</time></p><p class="title is-6"><a class="link-muted" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-TODO.html">信息系统实训 TODO</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a> / <a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/">信息系统实训</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-03-11T08:15:47.000Z">2020-03-11</time></p><p class="title is-6"><a class="link-muted" href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/TODO.html">TODO</a></p><p class="is-uppercase"></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Chrome/"><span class="level-start"><span class="level-item">Chrome</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Mac/"><span class="level-start"><span class="level-item">Mac</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/frontEnd/"><span class="level-start"><span class="level-item">frontEnd</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/go/"><span class="level-start"><span class="level-item">go</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/"><span class="level-start"><span class="level-item">java</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/java/Collections/"><span class="level-start"><span class="level-item">Collections</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/module-test/"><span class="level-start"><span class="level-item">module test</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/other/"><span class="level-start"><span class="level-item">other</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/taotao/"><span class="level-start"><span class="level-item">taotao</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">40</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">39</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">47</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/11/"><span class="level-start"><span class="level-item">November 2019</span></span><span class="level-end"><span class="level-item tag">69</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Tree/"><span class="tag">Tree</span><span class="tag is-grey-lightest">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag is-grey-lightest">22</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag is-grey-lightest">16</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Search/"><span class="tag">Binary Search</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Depth-first-Search/"><span class="tag">Depth-first Search</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hash-Table/"><span class="tag">Hash Table</span><span class="tag is-grey-lightest">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Two-Pointers/"><span class="tag">Two Pointers</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linked-List/"><span class="tag">Linked List</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Breadth-first-Search/"><span class="tag">Breadth-first Search</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tricks/"><span class="tag">tricks</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backtracking/"><span class="tag">Backtracking</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-Programming/"><span class="tag">Dynamic Programming</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bit-Manipulation/"><span class="tag">Bit Manipulation</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Design/"><span class="tag">Design</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Heap/"><span class="tag">Heap</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sort/"><span class="tag">Sort</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stack/"><span class="tag">Stack</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag is-grey-lightest">3</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=zronghui&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="zronghui" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div><p class="help">输入邮箱开始订阅，更博后邮件通知！</p></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://www.wntool.com/logo/image.php?output=png?fsize=35&amp;font=Snow.ttf&amp;text=zronghui&amp;mirror=No&amp;color=9933FF&amp;vcolor=3333FF&amp;bgcolor=FFFFFF&amp;alpha=yes&amp;output=png&amp;spacing=5&amp;shadow=no&amp;transparent=yes&amp;icon=no&amp;iconic=&amp;top_spacing=5&amp;left_spacing=6&amp;icon_size=48" alt="zronghui的博客" height="28"></a><p class="size-small"><span>&copy; 2020 zronghui</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>,Modify by <a href="https://github.com/removeif" target="_blank">removeif</a> <br>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br>    方便于网友与自己学习交流，如有侵权，请<a href="https://zronghui.github.io/message/" target="_blank">留言</a>，立即处理]<br><script type="text/javascript" src="/js/statistics.js"></script><span id="statistic-times"></span><br></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zronghui"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-Hans");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://zronghui.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>