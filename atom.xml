<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zronghui的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zronghui.github.io/"/>
  <updated>2020-03-12T14:24:26.000Z</updated>
  <id>https://zronghui.github.io/</id>
  
  <author>
    <name>zronghui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>信息系统实训 第1次作业</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC1%E6%AC%A1%E4%BD%9C%E4%B8%9A.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC1%E6%AC%A1%E4%BD%9C%E4%B8%9A.html</id>
    <published>2020-03-12T10:11:57.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">regex = <span class="string">r'1995年6月1日|1995/6/1|1995\-6\-1|1995\-06\-01|1995\-06'</span></span><br><span class="line">s = <span class="string">'•”xxx出生于1995年6月1日” • ”xxx出生于1995/6/1” • ”xxx出生于1995-6-1” • ”xxx出生于1995-06-01” • ”xxx出生于1995-06”'</span></span><br><span class="line">print(re.findall(regex, s))</span><br></pre></td></tr></table></figure><h2 id="运行截图"><a href="#运行截图" class="headerlink" title="运行截图"></a>运行截图</h2><img src="https://i.loli.net/2020/03/12/DTVoqjAr5ZBpt8w.png" alt="DTVoqjAr5ZBpt8w" style="zoom:50%;" />]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="研究生课程" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="信息系统实训" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/"/>
    
      <category term="提交作业" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/"/>
    
    
  </entry>
  
  <entry>
    <title>redis</title>
    <link href="https://zronghui.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/redis.html"/>
    <id>https://zronghui.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/redis.html</id>
    <published>2020-03-12T04:20:43.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="Mac："><a href="#Mac：" class="headerlink" title="Mac："></a>Mac：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br><span class="line"><span class="meta">#</span><span class="bash"> To have launchd start redis now and restart at login:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   brew services start redis</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Or, <span class="keyword">if</span> you don<span class="string">'t want/need a background service you can just run:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   redis-server /usr/<span class="built_in">local</span>/etc/redis.conf</span></span><br><span class="line">brew update;brew services start redis</span><br><span class="line"></span><br><span class="line">cotEdit /usr/local/etc/redis.conf</span><br><span class="line">注释 bind 127.0.0.1</span><br><span class="line">取消注释 requirepass foobare， 并配置密码</span><br><span class="line"></span><br><span class="line">brew services list                </span><br><span class="line"><span class="meta">#</span><span class="bash"> Name          Status  User         Plist</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> elasticsearch stopped</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis         started zhangronghui /Users/zhangronghui/Library/LaunchAgents/homebrew.mxcl.redis.plist</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> unbound       stopped</span></span><br><span class="line"></span><br><span class="line">brew services restart redis</span><br><span class="line">redis-cli -a redispassword</span><br></pre></td></tr></table></figure><h3 id="Linux-上："><a href="#Linux-上：" class="headerlink" title="Linux 上："></a>Linux 上：</h3><img src="https://i.loli.net/2020/03/12/CXDx73TjfPUGYwL.png" alt="CXDx73TjfPUGYwL" style="zoom:33%;" />]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://zronghui.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>前端工具</title>
    <link href="https://zronghui.github.io/frontEnd/%E5%89%8D%E7%AB%AF%E5%B7%A5%E5%85%B7.html"/>
    <id>https://zronghui.github.io/frontEnd/%E5%89%8D%E7%AB%AF%E5%B7%A5%E5%85%B7.html</id>
    <published>2020-03-11T14:58:26.000Z</published>
    <updated>2020-03-11T15:23:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><a href="https://enjoycss.com">Online CSS3 Code Generator With a Simple Graphical Interface - EnjoyCSS</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="frontEnd" scheme="https://zronghui.github.io/categories/frontEnd/"/>
    
    
  </entry>
  
  <entry>
    <title>信息系统实训 TODO</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-TODO.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-TODO.html</id>
    <published>2020-03-11T08:16:25.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p>sentry</p><p>Redis ?</p><p>swagger</p><p>查询<a href="https://book.douban.com/subject/34894380/?icn=index-topchart-subject">雾行者 (豆瓣)</a></p><p>封面挺好看</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="研究生课程" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="信息系统实训" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/"/>
    
    
  </entry>
  
  <entry>
    <title>TODO</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/TODO.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/TODO.html</id>
    <published>2020-03-11T08:15:47.000Z</published>
    <updated>2020-03-11T08:16:06.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>jiaoUrls</title>
    <link href="https://zronghui.github.io/jiao/jiaoUrls.html"/>
    <id>https://zronghui.github.io/jiao/jiaoUrls.html</id>
    <published>2020-03-11T07:29:19.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><a href="http://jyzy.luan.gov.cn/sky/index.html">六安市教育云平台</a><br><a href="http://tongbu.eduyun.cn/tbkt/tbktcz21/index.html">课程学习</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>小石潭记</title>
    <link href="https://zronghui.github.io/jiao/%E5%B0%8F%E7%9F%B3%E6%BD%AD%E8%AE%B0.html"/>
    <id>https://zronghui.github.io/jiao/%E5%B0%8F%E7%9F%B3%E6%BD%AD%E8%AE%B0.html</id>
    <published>2020-03-11T07:11:50.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>桃花源记</title>
    <link href="https://zronghui.github.io/jiao/%E6%A1%83%E8%8A%B1%E6%BA%90%E8%AE%B0.html"/>
    <id>https://zronghui.github.io/jiao/%E6%A1%83%E8%8A%B1%E6%BA%90%E8%AE%B0.html</id>
    <published>2020-03-11T07:11:42.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>时间的脚印</title>
    <link href="https://zronghui.github.io/jiao/%E6%97%B6%E9%97%B4%E7%9A%84%E8%84%9A%E5%8D%B0.html"/>
    <id>https://zronghui.github.io/jiao/%E6%97%B6%E9%97%B4%E7%9A%84%E8%84%9A%E5%8D%B0.html</id>
    <published>2020-03-11T07:11:34.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>大雁归来</title>
    <link href="https://zronghui.github.io/jiao/%E5%A4%A7%E9%9B%81%E5%BD%92%E6%9D%A5.html"/>
    <id>https://zronghui.github.io/jiao/%E5%A4%A7%E9%9B%81%E5%BD%92%E6%9D%A5.html</id>
    <published>2020-03-11T07:11:27.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>阿西莫夫短文两篇</title>
    <link href="https://zronghui.github.io/jiao/%E9%98%BF%E8%A5%BF%E8%8E%AB%E5%A4%AB%E7%9F%AD%E6%96%87%E4%B8%A4%E7%AF%87.html"/>
    <id>https://zronghui.github.io/jiao/%E9%98%BF%E8%A5%BF%E8%8E%AB%E5%A4%AB%E7%9F%AD%E6%96%87%E4%B8%A4%E7%AF%87.html</id>
    <published>2020-03-11T07:11:19.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>回延安</title>
    <link href="https://zronghui.github.io/jiao/%E5%9B%9E%E5%BB%B6%E5%AE%89.html"/>
    <id>https://zronghui.github.io/jiao/%E5%9B%9E%E5%BB%B6%E5%AE%89.html</id>
    <published>2020-03-11T07:11:07.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><img src="https://i.loli.net/2020/03/11/95I1GVNYheHrXz8.png" alt="95I1GVNYheHrXz8"></p><p><img src="https://i.loli.net/2020/03/11/fvLJAOayUSnw5CM.png" alt="fvLJAOayUSnw5CM"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>大自然的语言</title>
    <link href="https://zronghui.github.io/jiao/%E5%A4%A7%E8%87%AA%E7%84%B6%E7%9A%84%E8%AF%AD%E8%A8%80.html"/>
    <id>https://zronghui.github.io/jiao/%E5%A4%A7%E8%87%AA%E7%84%B6%E7%9A%84%E8%AF%AD%E8%A8%80.html</id>
    <published>2020-03-11T07:10:51.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>安塞腰鼓</title>
    <link href="https://zronghui.github.io/jiao/%E5%AE%89%E5%A1%9E%E8%85%B0%E9%BC%93.html"/>
    <id>https://zronghui.github.io/jiao/%E5%AE%89%E5%A1%9E%E8%85%B0%E9%BC%93.html</id>
    <published>2020-03-11T07:10:32.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>社戏</title>
    <link href="https://zronghui.github.io/jiao/%E7%A4%BE%E6%88%8F.html"/>
    <id>https://zronghui.github.io/jiao/%E7%A4%BE%E6%88%8F.html</id>
    <published>2020-03-11T07:10:20.000Z</published>
    <updated>2020-03-11T07:26:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><img src="https://i.loli.net/2020/03/11/Cyn8TXd16Zzl7Bk.png" alt="Cyn8TXd16Zzl7Bk"></p><p><img src="https://i.loli.net/2020/03/11/5NVabWjwtQK4oJU.png" alt="5NVabWjwtQK4oJU"></p><p><img src="https://i.loli.net/2020/03/11/1iCT6jguQwVhfNz.png" alt="1iCT6jguQwVhfNz"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="jiao" scheme="https://zronghui.github.io/categories/jiao/"/>
    
    
  </entry>
  
  <entry>
    <title>alfred</title>
    <link href="https://zronghui.github.io/Mac/07-alfred.html"/>
    <id>https://zronghui.github.io/Mac/07-alfred.html</id>
    <published>2020-03-10T05:01:41.000Z</published>
    <updated>2020-03-11T07:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><h2 id="workflow-search"><a href="#workflow-search" class="headerlink" title="workflow search"></a>workflow search</h2><p><a href="http://www.alfredworkflow.com/">Alfred 2 Workflow List | Search, Install and Share</a></p><h2 id="useful-workflow"><a href="#useful-workflow" class="headerlink" title="useful workflow"></a>useful workflow</h2><h3 id="DevDocs-v1-2-0-Download"><a href="#DevDocs-v1-2-0-Download" class="headerlink" title="DevDocs (v1.2.0) ~ Download"></a><a href="https://github.com/yannickglt/alfred-devdocs">DevDocs</a> (v1.2.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/devdocs/devdocs.alfredworkflow">Download</a></h3><p>Search through <a href="http://devdocs.io/">DevDocs</a> documentations by filtering the keyword for each language/library ~ <em>by <a href="https://github.com/yannickglt">@yannickglt</a>.</em></p><blockquote><p>Triggers: <code>cdoc</code>, <code>angular</code>, <code>coffeescript</code>, <code>css</code>, <code>d3</code>, <code>dom</code>, <code>dom_events</code>, <code>git</code>, <code>html</code>, <code>http</code>, <code>javascript</code>, <code>jquery</code>, <code>jqueryui</code>, <code>lodash</code>, <code>php</code>, <code>sass</code>, <code>backbone</code>, <code>c</code>, <code>cpp</code>, <code>chai</code>, <code>cordova</code>, <code>ember</code>, <code>express</code>, <code>go</code>, <code>grunt</code>, <code>haskell</code>, <code>jquerymobile</code>, <code>knockout</code>, <code>laravel</code>, <code>less</code>, <code>maxcdn</code>, <code>moment</code>, <code>node</code>, <code>postgresql</code>, <code>python</code>, <code>redis</code>, <code>requirejs</code>, <code>ruby</code>, <code>rails</code>, <code>sinon</code>, <code>underscore</code>, <code>yii</code>.</p></blockquote><img src="https://i.loli.net/2020/03/10/8gjeZf5NnSlV4Ro.png" alt="8gjeZf5NnSlV4Ro" style="zoom:33%;" /><img src="https://i.loli.net/2020/03/10/yCAK5dBMIcUElaj.png" alt="yCAK5dBMIcUElaj" style="zoom:33%;" /><img src="https://i.loli.net/2020/03/10/AfbnqMrwHaL3i5v.png" alt="AfbnqMrwHaL3i5v" style="zoom:33%;" /><h3 id="Caffeinate-v3-03-0-Download"><a href="#Caffeinate-v3-03-0-Download" class="headerlink" title="Caffeinate (v3.03.0) ~ Download"></a><a href="https://github.com/shawnrice/alfred-2-caffeinate-workflow">Caffeinate</a> (v3.03.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/caffeinate/caffeinate.alfredworkflow">Download</a></h3><p>Solves the problem of your computer constantly falling asleep by using OS X’s native command line ~ <em>by <a href="https://github.com/shawnrice/">@shawnrice</a>.</em></p><blockquote><p>Triggers: <code>caff</code>.</p></blockquote><img src="https://i.loli.net/2020/03/10/KiGHQqAyPSLlN1E.png" alt="KiGHQqAyPSLlN1E" style="zoom:33%;" /><h3 id="Emoji-v1-5-0-Download"><a href="#Emoji-v1-5-0-Download" class="headerlink" title="Emoji (v1.5.0) ~ Download"></a><a href="https://github.com/carlosgaldino/alfred-emoji-workflow">Emoji</a> (v1.5.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/emoji/emoji.alfredworkflow">Download</a></h3><p>Search for <a href="https://en.wikipedia.org/wiki/Emoji">Emojis</a> used by Basecamp, GitHub, GitLab, Trello, and other services ~ <em>by <a href="https://github.com/carlosgaldino/">@carlosgaldino</a>.</em></p><blockquote><p>Triggers: <code>emoji</code>, <code>emoji [alt key]</code>.</p></blockquote><img src="https://i.loli.net/2020/03/10/yKtOVnTFIoifZBS.png" alt="yKtOVnTFIoifZBS" style="zoom:33%;" /><h3 id="Encode-Decode-v1-8-0-Download"><a href="#Encode-Decode-v1-8-0-Download" class="headerlink" title="Encode/Decode (v1.8.0) ~ Download"></a><a href="https://github.com/willfarrell/alfred-encode-decode-workflow">Encode/Decode</a> (v1.8.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/encode-decode/encode-decode.alfredworkflow">Download</a></h3><p>Transform query strings through base64, html, url, and utf-8 encode/decode ~ <em>by <a href="https://github.com/willfarrell/">@willfarrell</a>.</em></p><blockquote><p>Triggers: <code>encode</code>, <code>decode</code>.</p></blockquote><img src="https://i.loli.net/2020/03/10/8uxcZXlPRQtTofe.png" alt="8uxcZXlPRQtTofe" style="zoom:33%;" /><h3 id="GitHub-v1-6-0-Download"><a href="#GitHub-v1-6-0-Download" class="headerlink" title="GitHub (v1.6.0) ~ Download"></a><a href="https://github.com/gharlan/alfred-github-workflow">GitHub</a> (v1.6.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/github/github.alfredworkflow">Download</a></h3><p>Easily open <a href="https://github.com/">GitHub</a> repositories and more in the browser ~ <em>by <a href="https://github.com/gharlan/">@gharlan</a>.</em></p><p>P.S.: You have to login before you can use the workflow: <code>gh &gt; login</code>.</p><blockquote><p>Triggers: <code>gh</code>.</p></blockquote><img src="https://cloud.githubusercontent.com/assets/398893/14360273/6d1d54ba-fcaa-11e5-99fb-a9b9976194e2.png" alt="github-1" style="zoom:33%;" /><img src="https://cloud.githubusercontent.com/assets/398893/14360270/6d1ae748-fcaa-11e5-80da-6433c312e452.png" alt="github-2" style="zoom:33%;" /><img src="https://cloud.githubusercontent.com/assets/398893/14360274/6d1eba8a-fcaa-11e5-8815-d7e9ca890542.png" alt="github-3" style="zoom:33%;" /><h3 id="Kill-Process-v1-2-0-Download"><a href="#Kill-Process-v1-2-0-Download" class="headerlink" title="Kill Process (v1.2.0) ~ Download"></a><a href="https://github.com/ngreenstein/alfred-process-killer">Kill Process</a> (v1.2.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/kill-process/kill-process.alfredworkflow">Download</a></h3><p>Easily find processes by name and kill them ~ <em>by <a href="https://github.com/ngreenstein">@ngreenstein</a>.</em></p><blockquote><p>Triggers: <code>kill</code>.</p></blockquote><img src="https://cloud.githubusercontent.com/assets/398893/14360276/6d2a33ba-fcaa-11e5-8fa5-4d3703a8129f.png" alt="kill" style="zoom:33%;" /><h3 id="Package-Managers-v3-16-0-Download"><a href="#Package-Managers-v3-16-0-Download" class="headerlink" title="Package Managers (v3.16.0) ~ Download"></a><a href="https://github.com/willfarrell/alfred-pkgman-workflow">Package Managers</a> (v3.16.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/package-managers/package-managers.alfredworkflow">Download</a></h3><p>Quick package/plugin/component lookup for your favorite package managers ~ <em>by <a href="https://github.com/willfarrell/">@willfarrell</a>.</em></p><blockquote><p>Triggers: <code>apt-get</code>, <code>apm</code>, <code>bower</code>, <code>brew</code>, <code>chef</code>, <code>cocoa</code>, <code>composer</code>, <code>docker</code>, <code>gems</code>, <code>gradle</code>, <code>grunt</code>, <code>gulp</code>, <code>hex</code>, <code>maven</code>, <code>npm</code>, <code>pear</code>, <code>puppet</code>, <code>pypi</code>, <code>raspbian</code>, <code>rpm</code>, <code>yo</code>.</p></blockquote><img src="https://cloud.githubusercontent.com/assets/398893/14360278/6d2d7f2a-fcaa-11e5-9463-0a909fd4a9bd.png" alt="pm-1" style="zoom:33%;" /><img src="https://cloud.githubusercontent.com/assets/398893/14360277/6d2c5c94-fcaa-11e5-964b-09633238e291.png" alt="pm-2" style="zoom:33%;" /><h3 id="Terminal-→-Finder-v1-6-0-Download"><a href="#Terminal-→-Finder-v1-6-0-Download" class="headerlink" title="Terminal → Finder (v1.6.0) ~ Download"></a><a href="https://github.com/LeEnno/alfred-terminalfinder">Terminal → Finder</a> (v1.6.0) ~ <a href="https://github.com/zenorocha/alfred-workflows/raw/master/terminal-finder/terminal-finder.alfredworkflow">Download</a></h3><p>Open current Finder (or Path Finder) window in Terminal (or iTerm) and vice versa ~ <em>by <a href="https://github.com/LeEnno/">@LeEnno</a>.</em></p><blockquote><p>Triggers: <code>ft</code>, <code>tf</code>, <code>fi</code>, <code>if</code>, <code>pt</code>, <code>tp</code>, <code>pi</code>, <code>ip</code>.</p></blockquote><img src="https://cloud.githubusercontent.com/assets/398893/14360282/6d3a0e3e-fcaa-11e5-8e5b-a8c5a3305962.png" alt="terminal-1" style="zoom: 33%;" /><img src="https://cloud.githubusercontent.com/assets/398893/14360284/6d3d19da-fcaa-11e5-933b-2ce62f83d77e.png" alt="terminal-2" style="zoom:33%;" /><h3 id="ssh-Download"><a href="#ssh-Download" class="headerlink" title="ssh (Download)"></a><a href="https://github.com/isometry/alfredworkflows/tree/master/net.isometry.alfred.ssh">ssh</a> (<a href="https://raw.github.com/isometry/alfredworkflows/master/ssh.alfredworkflow">Download</a>)</h3><p>by <a href="https://github.com/isometry">@isometry</a></p><img src="https://i.loli.net/2020/03/10/biwXPyrx3cWtsoY.png" alt="biwXPyrx3cWtsoY" style="zoom: 50%;" /><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://github.com/zenorocha/alfred-workflows">zenorocha/alfred-workflows: A collection of Alfred 3 and 4 workflows that will rock your world</a><br><a href="https://github.com/hzlzh/AlfredWorkflow.com">hzlzh/AlfredWorkflow.com: A public Collection of Alfred Workflows.</a><br><a href="https://github.com/deanishe/alfred-workflow">deanishe/alfred-workflow: Full-featured library for writing Alfred 3 &amp; 4 workflows</a><br><a href="https://github.com/willfarrell/alfred-workflows">willfarrell/alfred-workflows: Alfred Workflows for Developers</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mac" scheme="https://zronghui.github.io/categories/Mac/"/>
    
    
  </entry>
  
  <entry>
    <title>scrapy</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/scrapy.html</id>
    <published>2020-03-10T03:26:04.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html">Scrapy入门教程 — Scrapy 0.24.6 文档</a></p><h2 id="helloScrapy"><a href="#helloScrapy" class="headerlink" title="helloScrapy"></a>helloScrapy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line">scrapy startproject helloScrapy</span><br><span class="line">scrapy genspider volmoe volmoe.com</span><br><span class="line">scrapy genspider douban douban.com</span><br></pre></td></tr></table></figure><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><p>300 是权重，决定多个 pipeline 执行的顺序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ROBOTSTXT_OBEY = True</span></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'helloScrapy.pipelines.HelloscrapyPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理爬虫所爬取到的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file = open(<span class="string">'books.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 现将item数据转为字典类型，再将其保存为json文件</span></span><br><span class="line">        text = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">'\n'</span></span><br><span class="line">        <span class="comment"># 写入本地</span></span><br><span class="line">        self.file.write(text)</span><br><span class="line">        <span class="comment"># 会将item打印到屏幕上，方便观察</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫关闭时所执行的操作，在爬虫运行过程中只执行一次</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    book_name = scrapy.Field()</span><br><span class="line">    book_url = scrapy.Field()</span><br><span class="line">    book_desc = scrapy.Field()</span><br></pre></td></tr></table></figure><h3 id="Spider-volmoe-py"><a href="#Spider-volmoe-py" class="headerlink" title="Spider/volmoe.py"></a>Spider/volmoe.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> zhconv <span class="keyword">import</span> convert</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> helloScrapy.items <span class="keyword">import</span> BookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VolmoeSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'volmoe'</span></span><br><span class="line">    allowed_domains = [<span class="string">'volmoe.com'</span>]</span><br><span class="line">    <span class="comment"># start_urls = [f'https://volmoe.com/l/all,all,all,sortpoint,all,all/&#123;i&#125;.htm' for i in range(1, 2)]</span></span><br><span class="line">    start_urls = [<span class="string">f'https://volmoe.com/l/all,all,all,sortpoint,all,all/<span class="subst">&#123;i&#125;</span>.htm'</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">473</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自动调用 parse ，解析 item URL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        itemUrlList = list(set(response.xpath(<span class="string">'/html/body/div[7]/table'</span>) \</span><br><span class="line">                               .xpath(<span class="string">'//a/@href'</span>) \</span><br><span class="line">                               .re(<span class="string">r'https://volmoe.com/c/\d+.htm'</span>)))</span><br><span class="line">        <span class="keyword">for</span> itemUrl <span class="keyword">in</span> itemUrlList:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=itemUrl, callback=self.parse_item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 手动调用 parse_item ，解析 item</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            item = BookItem()</span><br><span class="line">            item[<span class="string">'book_url'</span>] = response.url</span><br><span class="line">            item[<span class="string">'book_name'</span>] = convert(response.xpath(<span class="string">'//div/b/text()'</span>).extract()[<span class="number">0</span>], <span class="string">'zh-cn'</span>)</span><br><span class="line">            item[<span class="string">'book_desc'</span>] = convert(response.xpath(<span class="string">'//*[@id="desc_text"]/text()'</span>).extract()[<span class="number">0</span>].strip(), <span class="string">'zh-cn'</span>)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol><li><p>爬取 列表页+详情页 用 rules 更方便，但是我用了，没有生效，所以手动指定函数处理</p></li><li><p>直接在网页上的 xpath 可能会与爬虫请求到的结构不同，所以可以用 scrapy shell “url”， view(response), 在网页上使用 xpath finder , xpath helper 查看指定元素的 xpath</p></li><li><p>scrapy genspider mydomain mydomain.com，最后的 mydomain.com 不用加 http 和 com 后面的/, 如 <del><a href="http://mydomain.com/">http://mydomain.com/</a></del></p></li><li><p>借助 <a href="https://github.com/further-reading/scrapy-gui">https://github.com/further-reading/scrapy-gui</a> 测试 CSS <del>xpath</del></p><img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" /><ol start="5"><li><p>获得的 URL 没有域名时：url = response.urljoin(next)  ?</p></li><li><p>可以在一个爬虫中用 custom_settings <img src="https://i.loli.net/2020/03/12/YLS5ftcphlDMTzx.png" alt="YLS5ftcphlDMTzx" style="zoom: 25%;" /></p></li><li><p>scrapy对request的URL去重 通过 yield scrapy.Request(url, self.parse, dont_filter=False) 里的 dont_filter（默认False） 实现</p></li><li><p>如果命令行里不想看到那么多输出的话，可以加个 -L WARNING 参数运行爬虫，如：scrapy crawl spider1 -L WARNING</p></li><li><p>scrapy shell 设置 UA</p><img src="https://i.loli.net/2020/03/12/H84sqghj3rNXpWv.jpg" alt="H84sqghj3rNXpWv" style="zoom:50%;" /></li></ol></li></ol><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><h3 id="启动所有爬虫？？"><a href="#启动所有爬虫？？" class="headerlink" title="启动所有爬虫？？"></a>启动所有爬虫？？</h3><h3 id="启动一个爬虫"><a href="#启动一个爬虫" class="headerlink" title="启动一个爬虫"></a>启动一个爬虫</h3><p>scrapy crawl volmoe</p><h3 id="事例-spider"><a href="#事例-spider" class="headerlink" title="事例 spider"></a>事例 spider</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</span><br><span class="line">            item = DmozItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'text()'</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>最简单存储爬取的数据的方式是使用 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/feed-exports.html#topics-feed-exports">Feed exports</a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dmoz -o items.json</span><br></pre></td></tr></table></figure><p>该命令将采用 <a href="http://en.wikipedia.org/wiki/JSON">JSON</a> 格式对爬取的数据进行序列化，生成 <code>items.json</code> 文件。</p><p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的item做更多更为复杂的操作，您可以编写 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/item-pipeline.html#topics-item-pipeline">Item Pipeline</a> 。 </p><h3 id="在Shell中尝试Selector选择器"><a href="#在Shell中尝试Selector选择器" class="headerlink" title="在Shell中尝试Selector选择器"></a>在Shell中尝试Selector选择器</h3><p>为了介绍Selector的使用方法，接下来我们将要使用内置的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html#topics-shell">Scrapy shell</a> 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p><p>您需要进入项目的根目录，执行下列命令来启动shell:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&quot;</span><br></pre></td></tr></table></figure><p>注解</p><p>当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 <code>&amp;</code> 字符)会导致Scrapy运行失败。</p><p>shell的输出类似:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[ ... Scrapy log here ... ]</span><br><span class="line"></span><br><span class="line">2014-01-23 17:11:42-0400 [default] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;www.dmoz.org&#x2F;Computers&#x2F;Programming&#x2F;Languages&#x2F;Python&#x2F;Books&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;</span><br><span class="line">[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span><br><span class="line">[s]   view(response)    View response in a browser</span><br><span class="line"></span><br><span class="line">In [1]:</span><br></pre></td></tr></table></figure><p>当shell载入后，您将得到一个包含response数据的本地 <code>response</code> 变量。输入 <code>response.body</code> 将输出response的包体， 输出 <code>response.headers</code> 可以看到response的包头。</p><p>更为重要的是，当输入 <code>response.selector</code> 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 <code>response.selector.xpath()</code> 、 <code>response.selector.css()</code> 的 快捷方法(shortcut): <code>response.xpath()</code> 和 <code>response.css()</code> 。</p><p>同时，shell根据response提前初始化了变量 <code>sel</code> 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。</p><p>让我们来试试:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: response.xpath(<span class="string">'//title'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [&lt;Selector xpath=<span class="string">'//title'</span> data=<span class="string">u'&lt;title&gt;Open Directory - Computers: Progr'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: response.xpath(<span class="string">'//title'</span>).extract()</span><br><span class="line">Out[<span class="number">2</span>]: [<span class="string">u'&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: response.xpath(<span class="string">'//title/text()'</span>)</span><br><span class="line">Out[<span class="number">3</span>]: [&lt;Selector xpath=<span class="string">'//title/text()'</span> data=<span class="string">u'Open Directory - Computers: Programming:'</span>&gt;]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: response.xpath(<span class="string">'//title/text()'</span>).extract()</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="string">u'Open Directory - Computers: Programming: Languages: Python: Books'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: response.xpath(<span class="string">'//title/text()'</span>).re(<span class="string">'(\w+):'</span>)</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="string">u'Computers'</span>, <span class="string">u'Programming'</span>, <span class="string">u'Languages'</span>, <span class="string">u'Python'</span>]</span><br></pre></td></tr></table></figure><h3 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h3><p>详见：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/commands.html">命令行工具(Command line tools) — Scrapy 0.24.6 文档</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以指定模板（怎么自定义模板？）</span></span><br><span class="line">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看 basic 模板</span></span><br><span class="line">scrapy genspider -d basic</span><br><span class="line"></span><br><span class="line">scrapy crawl myspider</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行contract检查, 运行没问题就好</span></span><br><span class="line">scrapy check [-l] &lt;spider&gt;</span><br><span class="line"></span><br><span class="line">scrapy list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 EDITOR 中设定的编辑器编辑给定的spider</span></span><br><span class="line">scrapy edit spider1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出（命令行？）</span></span><br><span class="line">scrapy fetch &lt;url&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ** 在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。</span></span><br><span class="line">scrapy view &lt;url&gt;</span><br><span class="line"></span><br><span class="line">scrapy shell [url]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。</span></span><br><span class="line">scrapy parse &lt;url&gt; [options]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取Scrapy的设定</span></span><br><span class="line">scrapy settings [options]</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get BOT_NAME</span></span><br><span class="line">scrapybot</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy settings --get DOWNLOAD_DELAY</span></span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在未创建项目的情况下，运行一个编写在Python文件中的spider。</span></span><br><span class="line">scrapy runspider &lt;spider_file.py&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy runspider myspider.py</span></span><br><span class="line">[ ... spider starts crawling ... ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。</span></span><br><span class="line">scrapy version [-v]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将项目部署到Scrapyd服务？</span></span><br><span class="line">scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行benchmark测试</span></span><br><span class="line">scrapy bench</span><br></pre></td></tr></table></figure><h3 id="items"><a href="#items" class="headerlink" title="items"></a>items</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个 item</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># item API和 dict API 非常相似</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一、与Item配合</span></span><br><span class="line"><span class="comment"># 创建item</span></span><br><span class="line">Product(name=<span class="string">'PC'</span>, price=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 获取字段的值</span></span><br><span class="line">p.get(<span class="string">'name'</span>)</span><br><span class="line">p.get(<span class="string">'name'</span>, <span class="string">'default'</span>)</span><br><span class="line">p[<span class="string">'name'</span>]</span><br><span class="line"><span class="string">'name'</span> <span class="keyword">in</span> product <span class="comment"># True  name 字段是否有填充</span></span><br><span class="line"><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields <span class="comment"># False  last_updated 是否是声明的字段</span></span><br><span class="line"><span class="comment"># 设置字段的值</span></span><br><span class="line">p[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></span><br><span class="line"><span class="comment"># 获取所有获取到的值</span></span><br><span class="line">p.keys() <span class="comment"># ['price', 'name']</span></span><br><span class="line">p.items() <span class="comment"># [('price', 1000), ('name', 'Desktop PC')]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他任务</span></span><br><span class="line"><span class="comment">#   复制item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product2 = Product(product)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product3 = product2.copy()</span><br><span class="line"><span class="comment">#   根据item创建字典(dict):</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dict(product) <span class="comment"># create a dict from all populated values</span></span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="number">1000</span>, <span class="string">'name'</span>: <span class="string">'Desktop PC'</span>&#125;</span><br><span class="line"><span class="comment">#   根据字典(dict)创建item:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Product(&#123;<span class="string">'name'</span>: <span class="string">'Laptop PC'</span>, <span class="string">'price'</span>: <span class="number">1500</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩展Item</span></span><br><span class="line"><span class="comment">#   可以通过继承原始的Item来扩展item(添加更多的字段或者修改某些字段的元数据)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscountedProduct</span><span class="params">(Product)</span>:</span></span><br><span class="line">    discount_percent = scrapy.Field(serializer=str)</span><br><span class="line">    discount_expiration_date = scrapy.Field()</span><br><span class="line"><span class="comment"># Item对象</span></span><br><span class="line"><span class="comment"># 字段(Field)对象</span></span><br></pre></td></tr></table></figure><h3 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//div/a/text()'</span>).extract()</span><br><span class="line">response.css(<span class="string">'div a::text'</span>).extract()</span><br><span class="line"><span class="comment"># 二者等价</span></span><br><span class="line">response.xpath(<span class="string">'//a/@href'</span>).extract_first()</span><br><span class="line">response.xpath(<span class="string">'a::attr(href)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复杂点的用法  href 属性包含 'image'</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "iamge")]/@href'</span>).extract()</span><br><span class="line">response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># xpath css 可以嵌套</span></span><br><span class="line">response.xpath(<span class="string">'...'</span>).css(<span class="string">'...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># re</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href, "image")]/text()'</span>).re(<span class="string">r'Name:\s*(.*)'</span>)</span><br><span class="line"><span class="comment"># re re_first</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 相对XPaths</span></span><br><span class="line"></span><br><span class="line">divs = response.xpath(<span class="string">'//div'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'//p'</span>):  <span class="comment"># this is wrong - gets all &lt;p&gt; from the whole document</span></span><br><span class="line"><span class="comment"># 下面是比较合适的处理方法(注意 .//p XPath的点前缀):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'.//p'</span>):  <span class="comment"># extracts all &lt;p&gt; inside</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> p.extract()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> p <span class="keyword">in</span> divs.xpath(<span class="string">'p'</span>):</span><br></pre></td></tr></table></figure><h3 id="spiders"><a href="#spiders" class="headerlink" title="spiders"></a>spiders</h3><h4 id="传递参数"><a href="#传递参数" class="headerlink" title="传递参数"></a>传递参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider -a category=electronics</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, category=None, *args, **kwargs)</span>:</span></span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category]</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h4 id="Spider-类"><a href="#Spider-类" class="headerlink" title="Spider 类"></a>Spider 类</h4><p><strong>name</strong></p><p>唯一标识 spider</p><p><strong>allowed_domains</strong></p><p>包含 spider 允许爬取的域名列表</p><p><strong>start_urls</strong></p><p><strong>start_requests() ?</strong></p><p>必须返回可迭代对象</p><p>逻辑：</p><ol><li>未指定 start_urls: start_requests 生效</li><li>指定 start_urls: make_requests_from_url 生效</li></ol><p>可以使用 start_requests 在启动时以 post 登录某个网站：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">"http://www.example.com/login"</span>,</span><br><span class="line">                               formdata=&#123;<span class="string">'user'</span>: <span class="string">'john'</span>, <span class="string">'pass'</span>: <span class="string">'secret'</span>&#125;,</span><br><span class="line">                               callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logged_in</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">    <span class="comment"># each of them, with another callback</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><strong>make_requests_from_url(url) ?</strong></p><p>该方法接受一个URL并返回用于爬取的 Request 对象。 该方法在初始化request时被 start_requests() 调用，也被用于转化url为request。</p><p><strong>log(message[, level, component])</strong></p><p>log中自动带上该spider的 name 属性。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'A response from %s just arrived!'</span> % response.url)</span><br></pre></td></tr></table></figure><p><strong>parse 中返回多个 request 和 item</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> MyItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        sel = scrapy.Selector(response)</span><br><span class="line">        <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">'//h3'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> MyItem(title=h3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br></pre></td></tr></table></figure><h4 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h4><p>继承自 Spider，有一个新属性 rules 和一个可复写的方法 parse_start_url</p><p>Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)</p><p><strong>当编写爬虫规则时，请避免使用 parse 作为回调函数</strong>。 由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果 您覆盖了 parse 方法，crawl spider 将会运行失败。</p><p>Follow 默认为 False</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True)</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'Hi, this is an item page! %s'</span> % response.url)</span><br><span class="line"></span><br><span class="line">        item = scrapy.Item()</span><br><span class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</span><br><span class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h4 id="XMLFeedSpider、CSVFeedSpider、SitemapSpider"><a href="#XMLFeedSpider、CSVFeedSpider、SitemapSpider" class="headerlink" title="XMLFeedSpider、CSVFeedSpider、SitemapSpider"></a>XMLFeedSpider、CSVFeedSpider、SitemapSpider</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">XMLFeedSpider例子</span><br><span class="line">该spider十分易用。下边是其中一个例子:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> XMLFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(XMLFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.xml'</span>]</span><br><span class="line">    iterator = <span class="string">'iternodes'</span> <span class="comment"># This is actually unnecessary, since it's the default value</span></span><br><span class="line">    itertag = <span class="string">'item'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_node</span><span class="params">(self, response, node)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a &lt;%s&gt; node!: %s'</span> % (self.itertag, <span class="string">''</span>.join(node.extract())))</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = node.xpath(<span class="string">'@id'</span>).extract()</span><br><span class="line">        item[<span class="string">'name'</span>] = node.xpath(<span class="string">'name'</span>).extract()</span><br><span class="line">        item[<span class="string">'description'</span>] = node.xpath(<span class="string">'description'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">CSVFeedSpider例子</span><br><span class="line">下面的例子和之前的例子很像，但使用了 CSVFeedSpider:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CSVFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CSVFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.csv'</span>]</span><br><span class="line">    delimiter = <span class="string">';'</span></span><br><span class="line">    headers = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'description'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_row</span><span class="params">(self, response, row)</span>:</span></span><br><span class="line">        log.msg(<span class="string">'Hi, this is a row!: %r'</span> % row)</span><br><span class="line"></span><br><span class="line">        item = TestItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = row[<span class="string">'id'</span>]</span><br><span class="line">        item[<span class="string">'name'</span>] = row[<span class="string">'name'</span>]</span><br><span class="line">        item[<span class="string">'description'</span>] = row[<span class="string">'description'</span>]</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">SitemapSpider样例</span><br><span class="line">简单的例子: 使用 parse 处理通过sitemap发现的所有url:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.example.com/sitemap.xml'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># ... scrape item here ...</span></span><br></pre></td></tr></table></figure><h3 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h3><p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。以下是item pipeline的一些典型应用：</p><ul><li>清理HTML数据</li><li>验证爬取的数据(检查item包含某些字段)</li><li>查重(并丢弃)</li><li>将爬取结果保存到数据库中</li></ul><h4 id="编写自己的-item-pipeline"><a href="#编写自己的-item-pipeline" class="headerlink" title="编写自己的 item pipeline"></a>编写自己的 item pipeline</h4><p><strong>process_item</strong></p><p>返回 item 或 抛出 DropItem 异常</p><p><strong>open_spider</strong></p><p>当spider被开启时，这个方法被调用。</p><p><strong>close_spider</strong><br>当spider被关闭时，这个方法被调用</p><h4 id="Item-pipeline-样例"><a href="#Item-pipeline-样例" class="headerlink" title="Item pipeline 样例"></a>Item pipeline 样例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证价格，同时丢弃没有价格的item</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br><span class="line">            </span><br><span class="line"><span class="comment"># 将item写入JSON文件</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">      </span><br><span class="line"><span class="comment"># 去重</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.ids_seen.add(item[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h4 id="代理ip"><a href="#代理ip" class="headerlink" title="代理ip"></a>代理ip</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span></span><br><span class="line">        self.logger.debug(<span class="string">'Get Exception'</span>)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = get_random_proxy() <span class="comment"># 形如 https://127.0.0.1:9743</span></span><br><span class="line">        <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure><p>settings.py 启动 item pipeline 插件</p><p>从小到大的顺序执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Jobs-暂停，恢复爬虫"><a href="#Jobs-暂停，恢复爬虫" class="headerlink" title="Jobs: 暂停，恢复爬虫"></a>Jobs: 暂停，恢复爬虫</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启用或恢复一个爬虫，都是</span></span><br><span class="line">scrapy crawl douban -s JOBDIR=jobs/douban-1</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://juejin.im/post/5aec1bb9f265da0b9526f855">Scrapy框架的使用之Selector的用法 - 掘金</a></p><p>介绍了 css xpath re 的用法</p><h2 id="工具-–-Useful-packages"><a href="#工具-–-Useful-packages" class="headerlink" title="工具 – Useful packages"></a>工具 – Useful packages</h2><p><a href="https://github.com/further-reading/scrapy-gui">further-reading/scrapy-gui: A simple, Qt-Webengine powered web browser with built in functionality for basic scrapy webscraping support.</a></p><p>今天在豆瓣失败了，大多还是很好用的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipython</span><br><span class="line"><span class="keyword">import</span> scrapy_gui</span><br><span class="line">scrapy_gui.open_browser()</span><br></pre></td></tr></table></figure><img src="https://i.loli.net/2020/03/11/b5X6KtGxNMn7OCv.png" alt="b5X6KtGxNMn7OCv" style="zoom:50%;" /><h3 id="gerapy"><a href="#gerapy" class="headerlink" title="gerapy"></a>gerapy</h3><p>主要用来管理本地或远程主机的 scrapy 项目，替代 scrapyd 的命令行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pip3 install gerapy</span><br><span class="line">gerapy init</span><br><span class="line">gerapy init &lt;workspace&gt; # 留空 为 gerapy</span><br><span class="line">cd gerapy</span><br><span class="line">gerapy migrate</span><br><span class="line">gerapy createsuperuser</span><br><span class="line">gerapy runserver</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Gerapy/Gerapy">Gerapy/Gerapy: Distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js</a><br><a href="https://ask.hellobi.com/blog/cuiqingcai/11195#articleHeader5">跟繁琐的命令行说拜拜！Gerapy分布式爬虫管理框架来袭！ - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a><br><a href="https://blog.csdn.net/baidu_32542573/article/details/79722390">python爬虫之Gerapy安装部署_Python_baidu_32542573的博客-CSDN博客</a></p><p>scrapy crawl shudan -s JOBDIR=jobs/shudan-1</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="研究生课程" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="信息系统实训" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/"/>
    
    
  </entry>
  
  <entry>
    <title>信息系统实训 第二次作业</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC2%E6%AC%A1%E4%BD%9C%E4%B8%9A.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD-%E7%AC%AC2%E6%AC%A1%E4%BD%9C%E4%B8%9A.html</id>
    <published>2020-03-10T00:45:02.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p>书籍搜索引擎</p><h2 id="任务一"><a href="#任务一" class="headerlink" title="任务一"></a>任务一</h2><h3 id="选择其中一个要爬取的网站"><a href="#选择其中一个要爬取的网站" class="headerlink" title="选择其中一个要爬取的网站"></a>选择其中一个要爬取的网站</h3><p><a href="https://volmoe.com/">Vol.moe [Kindle漫画|Kobo漫画|epub漫画]</a></p><h3 id="选择想爬取数据项（尽量全）"><a href="#选择想爬取数据项（尽量全）" class="headerlink" title="选择想爬取数据项（尽量全）"></a>选择想爬取数据项（尽量全）</h3><p>书籍名 书籍链接 书籍详情</p><img src="https://i.loli.net/2020/03/10/37r9Qo1dtHCmxLS.png" alt="37r9Qo1dtHCmxLS" style="zoom: 50%;" /><h3 id="确定数据项所在网页之间的跳转关系，绘制它们的url树"><a href="#确定数据项所在网页之间的跳转关系，绘制它们的url树" class="headerlink" title="确定数据项所在网页之间的跳转关系，绘制它们的url树"></a>确定数据项所在网页之间的跳转关系，绘制它们的url树</h3><p>说明：共 471 个分页，一个分页有 18 个详情页</p><img src="https://i.loli.net/2020/03/10/e1DENgTSL2FBsiv.png" alt="e1DENgTSL2FBsiv" style="zoom: 50%;" /><h3 id="写出url树的深度优先遍历、广度优先遍历顺序"><a href="#写出url树的深度优先遍历、广度优先遍历顺序" class="headerlink" title="写出url树的深度优先遍历、广度优先遍历顺序"></a>写出url树的深度优先遍历、广度优先遍历顺序</h3><p><strong>深度优先遍历</strong>: A B E F C G H D</p><p><strong>广度优先遍历</strong>: A B C D E F G H</p><h2 id="任务二"><a href="#任务二" class="headerlink" title="任务二"></a>任务二</h2><h3 id="选择要爬取的数据项所在的其中一个网页"><a href="#选择要爬取的数据项所在的其中一个网页" class="headerlink" title="选择要爬取的数据项所在的其中一个网页"></a>选择要爬取的数据项所在的其中一个网页</h3><p><a href="https://volmoe.com/c/50066.htm">灭鬼之刃: 吾峠呼世晴[Kindle漫画|epub漫画] [volmoe.com]</a></p><h3 id="对想爬取的数据项绘制出它们的dom树"><a href="#对想爬取的数据项绘制出它们的dom树" class="headerlink" title="对想爬取的数据项绘制出它们的dom树"></a>对想爬取的数据项绘制出它们的dom树</h3><p>Xpath:</p><p>书籍名：//div/b/text()</p><p>书籍详情：//*[@id=”desc_text”]/text()</p><p>爬取结果：</p><img src="https://i.loli.net/2020/03/12/vWZG6kezTg4HuKP.png" alt="vWZG6kezTg4HuKP" style="zoom: 50%;" />]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="研究生课程" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="信息系统实训" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/"/>
    
      <category term="提交作业" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A/"/>
    
    
  </entry>
  
  <entry>
    <title>书籍地址</title>
    <link href="https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E4%B9%A6%E7%B1%8D%E5%9C%B0%E5%9D%80.html"/>
    <id>https://zronghui.github.io/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/%E4%B9%A6%E7%B1%8D%E5%9C%B0%E5%9D%80.html</id>
    <published>2020-03-09T12:47:42.000Z</published>
    <updated>2020-03-12T14:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><p><a href="https://www.jiandaoyun.com/r/5b95200de22eed486e92ac63">电子书下载网站全览</a></p><ul><li><input checked="" disabled="" type="checkbox"> <p><a href="https://www.owllook.net/">owllook - 网络小说搜索引擎 - 最简洁清新的搜索阅读体验</a></p></li><li><input disabled="" type="checkbox"> <p><a href="https://www.shudan.vip/">书单网 - pdf - epub - mobi - azw3 让搜书变得简单</a> 集合多个书籍网站</p></li><li><input disabled="" type="checkbox"> <p><a href="http://pan.shudan.vip/">精品电子书下载 - 读好书，好读书！</a> 直接提供下载链接</p></li><li><input checked="" disabled="" type="checkbox"> <p><a href="https://volmoe.com/">Vol.moe [Kindle漫画|Kobo漫画|epub漫画] [volmoe.com]</a></p></li></ul><p><a href="https://www.zhihu.com/question/306450850">有哪些可以找免费电子书的网站? - 知乎</a><br><a href="https://www.zhihu.com/question/24007365">有哪些免费好用的电子书下载网站？ - 知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="研究生课程" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/"/>
    
      <category term="信息系统实训" scheme="https://zronghui.github.io/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%AE%AD/"/>
    
    
  </entry>
  
  <entry>
    <title>进程分析 端口分析</title>
    <link href="https://zronghui.github.io/Linux/%E8%BF%9B%E7%A8%8B%E5%88%86%E6%9E%90-%E7%AB%AF%E5%8F%A3%E5%88%86%E6%9E%90.html"/>
    <id>https://zronghui.github.io/Linux/%E8%BF%9B%E7%A8%8B%E5%88%86%E6%9E%90-%E7%AB%AF%E5%8F%A3%E5%88%86%E6%9E%90.html</id>
    <published>2020-03-08T08:43:36.000Z</published>
    <updated>2020-03-09T02:47:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><a id="more"></a><h2 id="进程分析"><a href="#进程分析" class="headerlink" title="进程分析"></a>进程分析</h2><h3 id="查看进程信息"><a href="#查看进程信息" class="headerlink" title="查看进程信息"></a>查看进程信息</h3><p>ps aux | grep python 查看系统中运行的 python 进程</p><h3 id="向进程发送信号"><a href="#向进程发送信号" class="headerlink" title="向进程发送信号"></a>向进程发送信号</h3><p>我们可以使用 kill PID 杀死一个进程，或者使用 kill -9 PID 强制杀死一个进程。</p><p> -9 是信号的一种，kill 命令会向进程发送一个信号，-9代表 SIGKILL 之意，用于强制终止某个进程</p><p>我们可以通过 kill -l 命令查看到所有的信号</p><p>HUP INT QUIT ILL TRAP ABRT BUS FPE KILL USR1 SEGV USR2 PIPE ALRM TERM STKFLT CHLD CONT STOP TSTP TTIN TTOU URG XCPU XFSZ VTALRM PROF WINCH POLL PWR SYS</p><p>上面的信号是有顺序的，比如第1个是 HUP，第9个是 KILL，下面两种方式是等价的：</p><p>kill -1 PID 和 kill -HUP PID</p><p>kill -9 PID 和 kill -KILL PID</p><p>信号HUP通常程序用这个信号进行优雅重载配置文件，重新启动并且不影响正在运行的服务。比如</p><p>pkill -1 uwsgi 优雅重启uwsgi 进程，对服务器没有影响</p><p>kill -1 NGINX_PID 优雅重启nginx进程，对服务器没有影响</p><p>除了知道可以这么使用之外，感兴趣的读者还可以自行学习，深入了解下uwsgi和nginx无损reload的机制。</p><p>CTRL-C 发送 SIGINT 信号给前台进程组中的所有进程，常用于终止正在运行的程序。</p><p>CTRL-Z 发送 SIGTSTP 信号给前台进程组中的所有进程，常用于挂起一个进程。</p><h3 id="查看进程打开了哪些文件"><a href="#查看进程打开了哪些文件" class="headerlink" title="查看进程打开了哪些文件"></a>查看进程打开了哪些文件</h3><p>sudo lsof -p PID</p><p>可以使用 lsof -p PID | grep TCP 查看进程中的 TCP 连接信息。</p><h3 id="查看文件被哪个进程使用"><a href="#查看文件被哪个进程使用" class="headerlink" title="查看文件被哪个进程使用"></a>查看文件被哪个进程使用</h3><p>使用这个命令查看一个文件被哪些进程正在使用 sudo lsof /path/to/file，示例如下：</p><blockquote><p>sudo lsof /home/tu/.virtualenvs/mic/bin/uwsgi</p></blockquote><h3 id="查看进程当前状态"><a href="#查看进程当前状态" class="headerlink" title="查看进程当前状态"></a>查看进程当前状态</h3><p>当我们发现一个进程启动了，端口也是正常的，但好像这个进程就是不“干活”。比如我们执行的是数据更新进程，这个进程不更新数据了，但还是在跑着。可能数据源有问题，可能我们写的程序有BUG，也可能是更新时要写入到的数据库出问题了（数据库连接不上了，写数据死锁了）。我们这里主要说下第二种，我们自己的程序如果有BUG，导致工作不正常，我们怎么知道它当前正在干什么呢，这时候就要用到Linux中的调试分析诊断strace，可以使用 sudo strace -p PID这个命令。</p><p>？</p><h2 id="端口分析"><a href="#端口分析" class="headerlink" title="端口分析"></a>端口分析</h2><p>比如我们在服务器上运行 Nginx，访问的时候就是连接不上，我们可以使用 ps aux | grep nginx看下nginx进程是不是启动了，也可以看下 80端口有没有被占用。换句话说，如果没有任何程序跑在这个端口上（或者说没有任何程序使用这个端口），证明忘了启动相关程序或者没能启动成功，或者说程序使用的端口被修改了，不是80了，那又怎么可能能访问到呢？</p><h3 id="查看全部端口占用情况"><a href="#查看全部端口占用情况" class="headerlink" title="查看全部端口占用情况"></a>查看全部端口占用情况</h3><p>Linux中我们可以使用 netstat 工具来进程网络分析，netstat 命令有非常多选项，这里只列出了常用的一部分</p><p>-a或–all 显示所有连接中的Socket，默认不显示 LISTEN 相关的。</p><p>-c或–continuous 持续列出网络状态，不断自动刷新输出。</p><p>-l或–listening 显示监听中的服务器的Socket。</p><p>-n或–numeric 直接使用IP地址，而不是展示域名。</p><p>-p或–programs 显示正在使用Socket的程序进程PID和名称。</p><p>-t或–tcp 显示TCP传输协议的连接。</p><p>-u或–udp 显示UDP传输协议的连接。</p><p>比如我们可以查看服务器中监控了哪些端口，如果我们的nginx是使用80端口，uwsgi使用的是7001端口，我们就能知道通过下面的命令</p><blockquote><p>netstat -nltp</p></blockquote><p>Active Internet connections (only servers)</p><p>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</p><p>tcp        0      0 0.0.0.0:7001            0.0.0.0:*               LISTEN      2070/uwsgi      </p><p>tcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      1575/redis-server 1</p><p>就能知道80端口的 nginx 是不是启动成功了，7001端口的uwsgi是不是启动成功了。</p><p>注意：如果PID和Program Name显示不出来，证明是权限不够，可以使用sudo运行</p><h3 id="查看具体端口占用情况"><a href="#查看具体端口占用情况" class="headerlink" title="查看具体端口占用情况"></a>查看具体端口占用情况</h3><blockquote><p>sudo lsof -i :80 (注意端口80前面有个英文的冒号)</p></blockquote><p>COMMAND    PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</p><p>nginx   4123   admin    3u  IPv4  13031      0t0  TCP *:http (LISTEN)</p><p>nginx   4124   admin    3u  IPv4  13031      0t0  TCP *:http (LISTEN)</p><p>我们可以通过这个方法查询出占用端口的程序，如果遇到端口已经被占用，原来的进程没有正确地终止，可以使用kill命令停掉原来的进程，这样我们就又可以使用这个端口了。</p><p>除了上面讲的一些命令，在部署过程中会经常用到下面的一些Linux命令，如果不清楚它们是做什么的，可以提前自行学习下这些Linux基础命令：</p><p>ls, touch, mkdir, mv, cp, ps, chmod, chown</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://zronghui.github.io/categories/Linux/"/>
    
    
  </entry>
  
</feed>
